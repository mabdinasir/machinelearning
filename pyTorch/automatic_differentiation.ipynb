{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Differentiation with torch.autograd\n",
    "\n",
    "Tensor: In PyTorch, the primary data structure is the Tensor, which is similar to NumPy arrays but with added capabilities for automatic differentiation.\n",
    "\n",
    "Requires Grad: When creating a tensor, you can specify requires_grad=True to indicate that you want PyTorch to track operations on this tensor. This is necessary if you want to compute gradients with respect to this tensor during backpropagation.\n",
    "\n",
    "Computation Graph: PyTorch constructs a computation graph dynamically as operations are performed on tensors. Nodes in this graph represent the operations, and edges represent the tensors flowing between operations. This graph is used to compute gradients.\n",
    "\n",
    "Backward Pass: To compute gradients, you call the backward() method on a tensor that represents the output of a computation. PyTorch then traverses the computation graph from this tensor, computing gradients and storing them in the .grad attribute of each tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z = <AddBackward0 object at 0x1210a18d0>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x1214a2ce0>\n",
      "w.grad tensor([[0.2996, 0.3159, 0.1794],\n",
      "        [0.2996, 0.3159, 0.1794],\n",
      "        [0.2996, 0.3159, 0.1794],\n",
      "        [0.2996, 0.3159, 0.1794],\n",
      "        [0.2996, 0.3159, 0.1794]])\n",
      "Arithmetic error tensor([0.2996, 0.3159, 0.1794])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.ones(5)  # input tensor\n",
    "y = torch.zeros(3)  # expected output\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w)+b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
    "\n",
    "print(f\"Gradient function for z = {z.grad_fn}\")\n",
    "print(f\"Gradient function for loss = {loss.grad_fn}\")\n",
    "\n",
    "loss.backward()\n",
    "print(\"w.grad\", w.grad)\n",
    "print(\"Arithmetic error\", b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requires grad: True\n",
      "requires grad: False\n"
     ]
    }
   ],
   "source": [
    "# Disabling Gradient Tracking\n",
    "z = torch.matmul(x, w)+b\n",
    "print(\"requires grad:\", z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w)+b\n",
    "print(\"requires grad:\", z.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# Another way to achieve the same result is to use the detach() method on the tensor:\n",
    "z = torch.matmul(x, w)+b\n",
    "z_det = z.detach()\n",
    "print(z_det.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computation graph for z: 15.0\n",
      "Gradient of x: 3.0\n",
      "Gradient of y: 8.0\n"
     ]
    }
   ],
   "source": [
    "# Create tensors with requires_grad=True to track computations\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "# Perform operations on tensors\n",
    "z = x * y + y ** 2\n",
    "\n",
    "# Print the computation graph\n",
    "print(f\"Computation graph for z: {z}\")\n",
    "\n",
    "# Perform backward pass to compute gradients\n",
    "z.backward()\n",
    "\n",
    "# Gradients are stored in the .grad attribute of tensors\n",
    "print(f\"Gradient of x: {x.grad}\")  # dz/dx\n",
    "print(f\"Gradient of y: {y.grad}\")  # dz/dy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A gradient is a measure of how a function changes as its input changes.\n",
    "\n",
    "# A computation graph is a visual and mathematical way to represent the sequence of operations that are performed to compute a function. In deep learning, it is used to represent the series of operations that transform the input data through various layers to produce the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
