{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Terminologies\n",
    "\n",
    "## Basic Concepts\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "A set of rules or steps used to solve a problem. In machine learning, algorithms process data to make predictions or decisions.\n",
    "\n",
    "### Model\n",
    "\n",
    "A mathematical representation created by a machine learning algorithm. It makes predictions or decisions based on input data.\n",
    "\n",
    "### Training\n",
    "\n",
    "The process of teaching a model to make predictions by feeding it data and adjusting its parameters.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "A collection of data used to train, validate, and test machine learning models.\n",
    "\n",
    "### Feature\n",
    "\n",
    "An individual measurable property or characteristic of a phenomenon being observed. Features are the input variables used in predictions.\n",
    "\n",
    "### Label\n",
    "\n",
    "The output variable or the target that the model is trying to predict.\n",
    "\n",
    "## Types of Learning\n",
    "\n",
    "### Supervised Learning\n",
    "\n",
    "A type of machine learning where the model is trained on labeled data, meaning the input comes with the correct output.\n",
    "\n",
    "### Unsupervised Learning\n",
    "\n",
    "A type of machine learning where the model is trained on data without labels and must find patterns or structure in the data.\n",
    "\n",
    "### Reinforcement Learning\n",
    "\n",
    "A type of machine learning where an agent learns to make decisions by receiving rewards or penalties based on its actions.\n",
    "\n",
    "## Model Evaluation\n",
    "\n",
    "### Classification\n",
    "\n",
    "A type of supervised learning where the model predicts a discrete label, such as \"spam\" or \"not spam\".\n",
    "\n",
    "### Regression\n",
    "\n",
    "A type of supervised learning where the model predicts a continuous value, such as house prices or stock prices.\n",
    "\n",
    "### Clustering\n",
    "\n",
    "A type of unsupervised learning where the model groups similar data points together, such as customer segmentation.\n",
    "\n",
    "### Overfitting\n",
    "\n",
    "When a model learns the training data too well, (basically memorizes), including noise and outliers, and performs poorly on new data.\n",
    "\n",
    "### Underfitting\n",
    "\n",
    "When a model is too simple to capture the underlying patterns in the data, leading to poor performance.\n",
    "\n",
    "### Validation Set\n",
    "\n",
    "A subset of the dataset used to tune model parameters and assess its performance during training.\n",
    "\n",
    "### Test Set\n",
    "\n",
    "A subset of the dataset used to evaluate the final model performance and generalization to new data.\n",
    "\n",
    "### Cross-Validation\n",
    "\n",
    "A technique for assessing how well a model generalizes by splitting the dataset into multiple training and validation sets.\n",
    "\n",
    "## Errors and Regularization\n",
    "\n",
    "### Bias\n",
    "\n",
    "Error introduced by approximating a real-world problem, which may be complex, by a simplified model.\n",
    "\n",
    "### Variance\n",
    "\n",
    "The model's sensitivity to changes in the training data. High variance can lead to overfitting.\n",
    "\n",
    "### Regularization\n",
    "\n",
    "Techniques used to prevent overfitting by adding a penalty to the loss function for large model parameters, such as L1 or L2 regularization.\n",
    "\n",
    "## Optimization\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "A function that measures the difference between the model's predictions and the actual labels. It guides the optimization process.\n",
    "\n",
    "### Optimization\n",
    "\n",
    "The process of adjusting the model's parameters to minimize the loss function.\n",
    "\n",
    "`optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)`\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "An optimization algorithm used to minimize the loss function by iteratively updating the model's parameters in the direction of the steepest descent.\n",
    "\n",
    "### Epoch\n",
    "\n",
    "One complete pass through the entire training dataset.\n",
    "\n",
    "### Batch Size\n",
    "\n",
    "The number of training examples used in one iteration of the optimization process.\n",
    "\n",
    "### Learning Rate\n",
    "\n",
    "A hyperparameter that controls the step size during gradient descent. It determines how quickly or slowly the model learns.\n",
    "\n",
    "### Hyperparameter\n",
    "\n",
    "Parameters that are set before training and control the training process, such as learning rate and batch size.\n",
    "\n",
    "## Neural Networks and Deep Learning\n",
    "\n",
    "### Neural Network\n",
    "\n",
    "A computational model inspired by the human brain, consisting of layers of interconnected nodes (neurons) that learn to recognize patterns in data.\n",
    "\n",
    "### Deep Learning\n",
    "\n",
    "A subset of machine learning involving neural networks with many layers (deep neural networks) that can learn complex patterns in large datasets.\n",
    "\n",
    "### Activation Function\n",
    "\n",
    "A function applied to a neuron's output in a neural network, introducing non-linearity and allowing the network to learn complex patterns.\n",
    "\n",
    "### Convolutional Neural Network (CNN)\n",
    "\n",
    "A type of neural network designed for processing structured grid data, such as images. A convolutional neural network (CNN) is a type of artificial neural network used primarily for image recognition and processing, due to its ability to recognize patterns in images.\n",
    "\n",
    "### Recurrent Neural Network (RNN)\n",
    "\n",
    "A type of neural network designed for sequential data, such as time series or natural language.\n",
    "\n",
    "### Transfer Learning\n",
    "\n",
    "A technique where a pre-trained model is used as the starting point for a new task, leveraging learned knowledge from the previous task.\n",
    "\n",
    "### Feature Engineering\n",
    "\n",
    "The process of selecting, modifying, and creating new features from raw data to improve model performance.\n",
    "\n",
    "### Dimensionality Reduction\n",
    "\n",
    "Techniques used to reduce the number of features in a dataset while preserving important information, such as PCA (Principal Component Analysis).\n",
    "\n",
    "## Advanced Concepts\n",
    "\n",
    "### Computation Graph\n",
    "\n",
    "A visual and mathematical way to represent the sequence of operations that are performed to compute a function. It is used in deep learning to represent the operations that transform input data through various layers to produce the output.\n",
    "\n",
    "### Automatic Differentiation\n",
    "\n",
    "A technique used to automatically compute the gradients of functions. In PyTorch, this is handled by `torch.autograd`, which tracks operations on tensors to enable gradient computation during backpropagation.\n",
    "\n",
    "### Gradient\n",
    "\n",
    "A measure of how a function changes as its input changes. In machine learning, the gradient represents the rate of change of the loss function with respect to the model's parameters.\n",
    "\n",
    "### Backpropagation\n",
    "\n",
    "A method used to compute the gradient of the loss function with respect to each parameter by applying the chain rule of calculus. It allows the optimizer to update the parameters in a way that reduces the loss.\n",
    "\n",
    "## Common Loss functions\n",
    "\n",
    "Common loss functions include nn.MSELoss (Mean Square Error) for regression tasks, and nn.NLLLoss (Negative Log Likelihood) for classification. nn.CrossEntropyLoss combines nn.LogSoftmax and nn.NLLLoss.\n",
    "\n",
    "## Additional Machine Learning Terminologies\n",
    "\n",
    "### Ensemble Learning\n",
    "\n",
    "A machine learning technique that combines multiple models to improve performance. Examples include Random Forests and Gradient Boosting Machines (GBMs).\n",
    "\n",
    "### Bagging\n",
    "\n",
    "A technique in ensemble learning where multiple models are trained independently on different subsets of the data, and their predictions are aggregated to make a final prediction.\n",
    "\n",
    "### Boosting\n",
    "\n",
    "A technique in ensemble learning where models are trained sequentially, with each model trying to correct the errors of its predecessors.\n",
    "\n",
    "### Dropout\n",
    "\n",
    "A regularization technique used in neural networks to prevent overfitting by randomly deactivating a fraction of neurons during training.\n",
    "\n",
    "### Batch Normalization\n",
    "\n",
    "A technique used in neural networks to normalize the activations of each layer, which can speed up training and improve performance.\n",
    "\n",
    "### Word Embedding\n",
    "\n",
    "A representation of words as dense vectors in a continuous vector space. Word embeddings capture semantic relationships between words and are commonly used in natural language processing tasks.\n",
    "\n",
    "### Attention Mechanism\n",
    "\n",
    "A mechanism used in neural networks, particularly in sequence-to-sequence models, to focus on relevant parts of the input sequence when making predictions.\n",
    "\n",
    "### Generative Adversarial Networks (GANs)\n",
    "\n",
    "A type of neural network architecture consisting of two networks, a generator and a discriminator, that are trained together in a competitive setting. GANs are used to generate new data samples that are similar to a given dataset.\n",
    "\n",
    "### Autoencoder\n",
    "\n",
    "A neural network architecture used for unsupervised learning, where the model learns to encode input data into a lower-dimensional representation and then decode it back to the original input.\n",
    "\n",
    "### Hyperparameter Tuning\n",
    "\n",
    "The process of finding the optimal values for hyperparameters to improve the performance of a machine learning model. Techniques include grid search, random search, and Bayesian optimization.\n",
    "\n",
    "### Precision and Recall\n",
    "\n",
    "Metrics used to evaluate the performance of classification models. Precision measures the proportion of true positives among all predicted positives, while recall measures the proportion of true positives among all actual positives.\n",
    "\n",
    "### F1 Score\n",
    "\n",
    "The harmonic mean of precision and recall, used as a single metric to evaluate the overall performance of a classification model.\n",
    "\n",
    "### ROC Curve and AUC\n",
    "\n",
    "Tools used to evaluate the performance of binary classification models. The ROC curve plots the true positive rate against the false positive rate at various thresholds, and the AUC (Area Under the Curve) measures the area under the ROC curve.\n",
    "\n",
    "### K-Means Clustering\n",
    "\n",
    "A popular clustering algorithm that partitions data into K clusters based on similarity. It aims to minimize the within-cluster variance.\n",
    "\n",
    "### Support Vector Machine (SVM)\n",
    "\n",
    "A supervised learning algorithm used for classification and regression tasks. SVM finds the hyperplane that best separates the data points of different classes in a high-dimensional space.\n",
    "\n",
    "### One-Hot Encoding\n",
    "\n",
    "A technique used to convert categorical variables into a numerical representation. Each category is represented by a binary vector, where only one element is 1 (hot) and the rest are 0 (cold).\n",
    "\n",
    "### Cross-Entropy Loss\n",
    "\n",
    "A loss function commonly used in classification tasks, particularly when the output of the model is a probability distribution over multiple classes.\n",
    "\n",
    "### Word2Vec\n",
    "\n",
    "A popular word embedding technique that learns vector representations of words by predicting the surrounding words in a text corpus.\n",
    "\n",
    "### Long Short-Term Memory (LSTM)\n",
    "\n",
    "A type of recurrent neural network architecture designed to capture long-term dependencies in sequential data, such as time series or natural language.\n",
    "\n",
    "### Transformer\n",
    "\n",
    "A deep learning architecture based solely on self-attention mechanisms, used primarily in natural language processing tasks such as machine translation and language modeling.\n",
    "\n",
    "### Reinforcement Learning Terms\n",
    "\n",
    "-   **Policy**: A strategy or rule that an agent uses to make decisions in a reinforcement learning environment.\n",
    "-   **Value Function**: A function that estimates the expected return (total reward) of following a particular policy.\n",
    "-   **Q-Learning**: A model-free reinforcement learning algorithm that learns the value of taking an action in a particular state.\n",
    "-   **Exploration vs. Exploitation**: The trade-off between trying out new actions (exploration) and selecting actions with the highest known rewards (exploitation) in reinforcement learning.\n",
    "-   **Markov Decision Process (MDP)**: A mathematical framework used to model decision-making processes in reinforcement learning, consisting of states, actions, transition probabilities, and rewards.\n",
    "\n",
    "### Natural Language Processing (NLP) Terms\n",
    "\n",
    "-   **Tokenization**: The process of breaking down a text into smaller units, such as words or subwords, for further analysis.\n",
    "-   **N-gram**: A contiguous sequence of N items (typically words or characters) in a text.\n",
    "-   **TF-IDF (Term Frequency-Inverse Document Frequency)**: A numerical statistic that reflects the importance of a word in a document relative to a collection of documents.\n",
    "-   **Named Entity Recognition (NER)**: A task in NLP that involves identifying and classifying named entities (such as names of people, organizations, and locations) in a text.\n",
    "-   **Word Sense Disambiguation**: The task of determining the correct meaning (sense) of a word in context, especially when the word has multiple meanings.\n",
    "\n",
    "### Layer\n",
    "\n",
    "A layer consists of small individual units called neurons. A neuron in a neural network can be better understood with the help of biological neurons. An artificial neuron is similar to a biological neuron. It receives input from the other neurons, performs some processing, and produces an output\n",
    "\n",
    "### ReLU - Rectified linear unit - ReLU(x)=max(0,x)\n",
    "\n",
    "    Also known as the rectifier activation function\n",
    "\n",
    "Unlike the sigmoid and tanh functions, ReLU is a non-saturating function, which means that it does not become flat at the extremes of the input range\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
