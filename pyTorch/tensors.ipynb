{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensors\n",
    "\n",
    "# Tensors are multi-dimensional arrays that can store numerical data. They are the primary data structure in PyTorch and are used for computations in neural networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor\n",
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "z = torch.zeros(5, 3) # 5x3 tensor with zeros\n",
    "print(z)\n",
    "print(z.dtype) # default data type is float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int16)\n"
     ]
    }
   ],
   "source": [
    "i = torch.ones((5, 3), dtype=torch.int16) # 5x3 tensor with ones, and data type is int16(integer 16 bits)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When working with random numbers in machine learning or other applications, it is often important to have reproducible results. By setting a seed, you can ensure that the sequence of random numbers generated will be the same every time you run your code, which can be useful for debugging or comparing different runs of an experiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A random tensor:\n",
      "tensor([[0.3126, 0.3791],\n",
      "        [0.3087, 0.0736]])\n",
      "\n",
      "A different random tensor:\n",
      "tensor([[0.4216, 0.0691],\n",
      "        [0.2332, 0.4047]])\n",
      "\n",
      "Should match r1:\n",
      "tensor([[0.3126, 0.3791],\n",
      "        [0.3087, 0.0736]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1729) # seed for random number generator\n",
    "r1 = torch.rand(2, 2)\n",
    "print('A random tensor:')\n",
    "print(r1)\n",
    "\n",
    "r2 = torch.rand(2, 2)\n",
    "print('\\nA different random tensor:')\n",
    "print(r2) # new values\n",
    "\n",
    "torch.manual_seed(1729)\n",
    "r3 = torch.rand(2, 2)\n",
    "print('\\nShould match r1:')\n",
    "print(r3) # repeats values of r1 because of re-seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "tensor([[3., 3., 3.],\n",
      "        [3., 3., 3.]])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "ones = torch.ones(2, 3)\n",
    "print(ones)\n",
    "\n",
    "twos = torch.ones(2, 3) * 2 # every element is multiplied by 2\n",
    "print(twos)\n",
    "\n",
    "threes = ones + twos       # addition allowed because shapes are similar\n",
    "print(threes)              # tensors are added element-wise\n",
    "print(threes.shape)        # this has the same dimensions as input tensors\n",
    "\n",
    "r1 = torch.rand(2, 3)\n",
    "r2 = torch.rand(3, 2)\n",
    "# uncomment this line to get a runtime error\n",
    "# r3 = r1 + r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here’s a small sample of the mathematical operations available:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A random matrix, r:\n",
      "tensor([[ 0.9956, -0.2232],\n",
      "        [ 0.3858, -0.6593]])\n",
      "\n",
      "Absolute value of r:\n",
      "tensor([[0.9956, 0.2232],\n",
      "        [0.3858, 0.6593]])\n",
      "\n",
      "Inverse sine of r:\n",
      "tensor([[ 1.4775, -0.2251],\n",
      "        [ 0.3961, -0.7199]])\n",
      "\n",
      "Determinant of r:\n",
      "tensor(-0.5703)\n",
      "\n",
      "Singular value decomposition of r:\n",
      "torch.return_types.svd(\n",
      "U=tensor([[-0.8353, -0.5497],\n",
      "        [-0.5497,  0.8353]]),\n",
      "S=tensor([1.1793, 0.4836]),\n",
      "V=tensor([[-0.8851, -0.4654],\n",
      "        [ 0.4654, -0.8851]]))\n",
      "\n",
      "Average and standard deviation of r:\n",
      "(tensor(0.7217), tensor(0.1247))\n",
      "\n",
      "Maximum value of r:\n",
      "tensor(0.9956)\n",
      "Common functions:\n",
      "tensor([[0.7231, 0.0482, 0.4961, 0.9278],\n",
      "        [0.0124, 0.6939, 0.4823, 0.4587]])\n",
      "tensor([[-0., -0., 1., -0.],\n",
      "        [1., 1., -0., -0.]])\n",
      "tensor([[-1., -1.,  0., -1.],\n",
      "        [ 0.,  0., -1., -1.]])\n",
      "tensor([[-0.5000, -0.0482,  0.4961, -0.5000],\n",
      "        [ 0.0124,  0.5000, -0.4823, -0.4587]])\n",
      "\n",
      "Sine and arcsine:\n",
      "tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
      "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n",
      "tensor([0.0000, 0.7854, 1.5708, 0.7854])\n",
      "\n",
      "Bitwise XOR:\n",
      "tensor([3, 2, 1])\n",
      "\n",
      "Broadcasted, element-wise equality comparison:\n",
      "tensor([[ True, False],\n",
      "        [False, False]])\n",
      "\n",
      "Reduction ops:\n",
      "tensor(4.)\n",
      "4.0\n",
      "tensor(2.5000)\n",
      "tensor(1.2910)\n",
      "tensor(24.)\n",
      "tensor([1, 2])\n",
      "\n",
      "Vectors & Matrices:\n",
      "tensor([ 0.,  0., -1.])\n",
      "tensor([[0.4115, 0.6839],\n",
      "        [0.0703, 0.5105]])\n",
      "tensor([[1.2344, 2.0516],\n",
      "        [0.2108, 1.5315]])\n",
      "torch.return_types.svd(\n",
      "U=tensor([[-0.8488, -0.5287],\n",
      "        [-0.5287,  0.8488]]),\n",
      "S=tensor([2.8022, 0.5204]),\n",
      "V=tensor([[-0.4137, -0.9104],\n",
      "        [-0.9104,  0.4137]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t2/lz6cdn7s5dbf6m4_78hwndn00000gn/T/ipykernel_77438/2021271175.py:70: UserWarning: Using torch.cross without specifying the dim arg is deprecated.\n",
      "Please either pass the dim explicitly or simply use torch.linalg.cross.\n",
      "The default value of dim will change to agree with that of linalg.cross in a future release. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1712608635429/work/aten/src/ATen/native/Cross.cpp:66.)\n",
      "  print(torch.cross(v2, v1)) # negative of z unit vector (v1 x v2 == -v2 x v1)\n"
     ]
    }
   ],
   "source": [
    "r = (torch.rand(2, 2) - 0.5) * 2 # values between -1 and 1\n",
    "print('A random matrix, r:')\n",
    "print(r)\n",
    "\n",
    "# Common mathematical operations are supported:\n",
    "print('\\nAbsolute value of r:')\n",
    "print(torch.abs(r))\n",
    "\n",
    "# ...as are trigonometric functions:\n",
    "print('\\nInverse sine of r:')\n",
    "print(torch.asin(r))\n",
    "\n",
    "# ...and linear algebra operations like determinant and singular value decomposition\n",
    "print('\\nDeterminant of r:')\n",
    "print(torch.det(r))\n",
    "print('\\nSingular value decomposition of r:')\n",
    "print(torch.svd(r))\n",
    "\n",
    "# ...and statistical and aggregate operations:\n",
    "print('\\nAverage and standard deviation of r:')\n",
    "print(torch.std_mean(r))\n",
    "print('\\nMaximum value of r:')\n",
    "print(torch.max(r))\n",
    "\n",
    "# common functions\n",
    "a = torch.rand(2, 4) * 2 - 1\n",
    "print('Common functions:')\n",
    "print(torch.abs(a))\n",
    "print(torch.ceil(a))\n",
    "print(torch.floor(a))\n",
    "print(torch.clamp(a, -0.5, 0.5)) # clamp values to a range\n",
    "\n",
    "# trigonometric functions and their inverses\n",
    "angles = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])\n",
    "sines = torch.sin(angles)\n",
    "inverses = torch.asin(sines)\n",
    "print('\\nSine and arcsine:')\n",
    "print(angles)\n",
    "print(sines)\n",
    "print(inverses)\n",
    "\n",
    "# bitwise operations\n",
    "print('\\nBitwise XOR:')\n",
    "b = torch.tensor([1, 5, 11])\n",
    "c = torch.tensor([2, 7, 10])\n",
    "print(torch.bitwise_xor(b, c))\n",
    "\n",
    "# comparisons:\n",
    "print('\\nBroadcasted, element-wise equality comparison:')\n",
    "d = torch.tensor([[1., 2.], [3., 4.]])\n",
    "e = torch.ones(1, 2)  # many comparison ops support broadcasting!\n",
    "print(torch.eq(d, e)) # returns a tensor of type bool\n",
    "\n",
    "# reductions:\n",
    "print('\\nReduction ops:')\n",
    "print(torch.max(d))        # returns a single-element tensor\n",
    "print(torch.max(d).item()) # extracts the value from the returned tensor\n",
    "print(torch.mean(d))       # average\n",
    "print(torch.std(d))        # standard deviation\n",
    "print(torch.prod(d))       # product of all numbers\n",
    "print(torch.unique(torch.tensor([1, 2, 1, 2, 1, 2]))) # filter unique elements\n",
    "\n",
    "# vector and linear algebra operations\n",
    "v1 = torch.tensor([1., 0., 0.])         # x unit vector\n",
    "v2 = torch.tensor([0., 1., 0.])         # y unit vector\n",
    "m1 = torch.rand(2, 2)                   # random matrix\n",
    "m2 = torch.tensor([[3., 0.], [0., 3.]]) # three times identity matrix\n",
    "\n",
    "print('\\nVectors & Matrices:')\n",
    "print(torch.cross(v2, v1)) # negative of z unit vector (v1 x v2 == -v2 x v1)\n",
    "print(m1)\n",
    "m3 = torch.matmul(m1, m2)\n",
    "print(m3)                  # 3 times m1\n",
    "print(torch.svd(m3))       # singular value decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shapes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3])\n",
      "tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[0.9451, 0.2359, 0.1979],\n",
      "         [0.3327, 0.6146, 0.5999]],\n",
      "\n",
      "        [[0.5013, 0.9397, 0.8656],\n",
      "         [0.5207, 0.6865, 0.3614]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(2, 2, 3) # uninitialized tensor, reserves memory but does not initialize values\n",
    "print(x.shape) # 2x2x3 tensor\n",
    "print(x) \n",
    "\n",
    "empty_like_x = torch.empty_like(x) # uninitialized tensor with the same shape as x\n",
    "print(empty_like_x.shape) # 2x2x3 tensor\n",
    "print(empty_like_x)\n",
    "\n",
    "zeros_like_x = torch.zeros_like(x) # tensor with zeros and the same shape as x\n",
    "print(zeros_like_x.shape) # 2x2x3 tensor\n",
    "print(zeros_like_x)\n",
    "\n",
    "ones_like_x = torch.ones_like(x) # tensor with ones and the same shape as x\n",
    "print(ones_like_x.shape) # 2x2x3 tensor\n",
    "print(ones_like_x)\n",
    "\n",
    "rand_like_x = torch.rand_like(x) # tensor with random values and the same shape as x\n",
    "print(rand_like_x.shape) # 2x2x3 tensor\n",
    "print(rand_like_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify its data directly from a PyTorch collection:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.1416, 2.7183],\n",
      "        [1.6180, 0.0073]])\n",
      "tensor([ 2,  3,  5,  7, 11, 13, 17, 19])\n",
      "tensor([[2, 4, 6],\n",
      "        [3, 6, 9]])\n"
     ]
    }
   ],
   "source": [
    "some_constants = torch.tensor([[3.1415926, 2.71828], [1.61803, 0.0072897]]) # tensor with specific values\n",
    "print(some_constants) # 2x2 tensor\n",
    "\n",
    "some_integers = torch.tensor((2, 3, 5, 7, 11, 13, 17, 19)) \n",
    "print(some_integers) # 1D tensor\n",
    "\n",
    "more_integers = torch.tensor(((2, 4, 6), [3, 6, 9])) \n",
    "print(more_integers) # 2x3 tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Types\n",
    "\n",
    "### Setting the datatype of a tensor is possible a couple of ways:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int16)\n",
      "tensor([[ 3.8937, 16.1945, 12.3902],\n",
      "        [15.0541, 16.8179, 17.7174]], dtype=torch.float64)\n",
      "tensor([[ 3, 16, 12],\n",
      "        [15, 16, 17]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones((2, 3), dtype=torch.int16) # 2x3 tensor with ones, and data type is int16(integer 16 bits)\n",
    "print(a)\n",
    "\n",
    "b = torch.rand((2, 3), dtype=torch.float64) * 20. # 2x3 tensor with random values between 0 and 20, and data type is float64(double precision)\n",
    "print(b)\n",
    "\n",
    "c = b.to(torch.int32) # convert b to int32(integer 32 bits)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Math with tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[2., 2.],\n",
      "        [2., 2.]])\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]])\n",
      "tensor([[4., 4.],\n",
      "        [4., 4.]])\n",
      "tensor([[1.4142, 1.4142],\n",
      "        [1.4142, 1.4142]])\n",
      "tensor([[ 2.,  4.],\n",
      "        [ 8., 16.]])\n",
      "tensor([[5., 5.],\n",
      "        [5., 5.]])\n",
      "tensor([[12., 12.],\n",
      "        [12., 12.]])\n"
     ]
    }
   ],
   "source": [
    "ones = torch.zeros(2, 2) + 1  # 2x2 tensor with ones\n",
    "\n",
    "twos = torch.ones(2, 2) * 2   # 2x2 tensor with twos\n",
    "\n",
    "threes = (torch.ones(2, 2) * 7 - 1) / 2  # 2x2 tensor with threes\n",
    "\n",
    "fours = twos ** 2 # element-wise squaring\n",
    "\n",
    "sqrt2s = twos ** 0.5 # element-wise square root\n",
    "\n",
    "powers2 = twos ** torch.tensor([[1, 2], [3, 4]]) # element-wise exponentiation\n",
    "\n",
    "fives = ones + fours # element-wise addition\n",
    "\n",
    "dozens = threes * fours # element-wise multiplication\n",
    "\n",
    "print(ones)\n",
    "print(twos)\n",
    "print(threes)\n",
    "print(fours)\n",
    "print(sqrt2s)\n",
    "print(powers2)\n",
    "print(fives)\n",
    "print(dozens)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The exception to the same-shapes rule is tensor broadcasting.\n",
    "\n",
    "#### Broadcasting is a way to perform an operation between tensors that have similarities in their shapes. In the example below, the one-row, four-column tensor is multiplied by both rows of the two-row, four-column tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2138, 0.5395, 0.3686, 0.4007],\n",
      "        [0.7220, 0.8217, 0.2612, 0.7375]])\n",
      "tensor([[0.4276, 1.0791, 0.7371, 0.8014],\n",
      "        [1.4439, 1.6434, 0.5224, 1.4750]])\n"
     ]
    }
   ],
   "source": [
    "rand = torch.rand(2, 4)\n",
    "doubled = rand * (torch.ones(1, 4) * 2)\n",
    "\n",
    "print(rand)\n",
    "print(doubled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Altering Tensors in Place\n",
    "\n",
    "#### Most binary operations on tensors will return a third, new tensor. When we say c = a \\* b (where a and b are tensors), the new tensor c will occupy a region of memory distinct from the other tensors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most of the math functions have a version with an appended underscore (\\_) that will alter a tensor in place.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:\n",
      "tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
      "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n",
      "tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
      "\n",
      "b:\n",
      "tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
      "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n",
      "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])\n",
    "print('a:')\n",
    "print(a)\n",
    "print(torch.sin(a))   # this operation creates a new tensor in memory\n",
    "print(a)              # a has not changed\n",
    "\n",
    "b = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])\n",
    "print('\\nb:')\n",
    "print(b)\n",
    "print(torch.sin_(b))  # note the underscore, this operation modifies b in place\n",
    "print(b)              # b has changed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For arithmetic operations, there are functions that behave similarly:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[0.8328, 0.8444],\n",
      "        [0.2941, 0.3788]])\n",
      "\n",
      "After adding:\n",
      "tensor([[1.8328, 1.8444],\n",
      "        [1.2941, 1.3788]])\n",
      "tensor([[1.8328, 1.8444],\n",
      "        [1.2941, 1.3788]])\n",
      "tensor([[0.8328, 0.8444],\n",
      "        [0.2941, 0.3788]])\n",
      "\n",
      "After multiplying\n",
      "tensor([[0.6936, 0.7130],\n",
      "        [0.0865, 0.1435]])\n",
      "tensor([[0.6936, 0.7130],\n",
      "        [0.0865, 0.1435]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2, 2)\n",
    "b = torch.rand(2, 2)\n",
    "\n",
    "print('Before:')\n",
    "print(a)\n",
    "print(b)\n",
    "print('\\nAfter adding:')\n",
    "print(a.add_(b))\n",
    "print(a)\n",
    "print(b)\n",
    "print('\\nAfter multiplying')\n",
    "print(b.mul_(b))\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There is another option for placing the result of a computation in an existing, allocated tensor. Many of the methods and functions we’ve seen so far - including creation methods! - have an out argument that lets you specify a tensor to receive the output. If the out tensor is the correct shape and dtype, this can happen without a new memory allocation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n",
      "tensor([[0.0929, 0.0261],\n",
      "        [0.4759, 0.3034]])\n",
      "tensor([[0.9883, 0.4762],\n",
      "        [0.7242, 0.0776]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2, 2)\n",
    "b = torch.rand(2, 2)\n",
    "c = torch.zeros(2, 2)\n",
    "old_id = id(c)\n",
    "\n",
    "print(c)\n",
    "d = torch.matmul(a, b, out=c)\n",
    "print(c)                # contents of c have changed\n",
    "\n",
    "assert c is d           # test c & d are same object, not just containing equal values\n",
    "assert id(c) == old_id  # make sure that our new c is the same object as the old one\n",
    "\n",
    "torch.rand(2, 2, out=c) # works for creation too!\n",
    "print(c)                # c has changed again\n",
    "assert id(c) == old_id  # still the same object!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copying Tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  1., 561.],\n",
      "        [  1.,   1.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2, 2)\n",
    "b = a\n",
    "\n",
    "a[0][1] = 561  # we change a...\n",
    "print(b)       # ...and b is also altered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But what if you want a separate copy of the data to work on? The clone() method is there for you:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True, True],\n",
      "        [True, True]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[0.4004, 0.9877],\n",
      "        [0.0352, 0.0905]], requires_grad=True)\n",
      "tensor([[0.4004, 0.9877],\n",
      "        [0.0352, 0.0905]], grad_fn=<CloneBackward0>)\n",
      "tensor([[0.4004, 0.9877],\n",
      "        [0.0352, 0.0905]])\n",
      "tensor([[0.4004, 0.9877],\n",
      "        [0.0352, 0.0905]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2, 2)\n",
    "b = a.clone()\n",
    "\n",
    "assert b is not a      # different objects in memory...\n",
    "print(torch.eq(a, b))  # ...but still with the same contents!\n",
    "\n",
    "a[0][1] = 561          # a changes...\n",
    "print(b)               # ...but b is still all ones\n",
    "\n",
    "# Turn on autograd\n",
    "a = torch.rand(2, 2, requires_grad=True) # turn on autograd => computation history tracking is turned on.\n",
    "print(a)\n",
    "\n",
    "b = a.clone()\n",
    "print(b)\n",
    "\n",
    "c = a.detach().clone() # detach => computation history tracking is turned off. The detach() method detaches the tensor from its computation history\n",
    "print(c)\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulating Tensor Shapes\n",
    "\n",
    "### Changing the Number of Dimensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 226, 226])\n",
      "torch.Size([1, 3, 226, 226])\n",
      "tensor([[[[[0.9177]]]]])\n",
      "torch.Size([1, 20])\n",
      "tensor([[0.1380, 0.0226, 0.3025, 0.1448, 0.5758, 0.8992, 0.2347, 0.1899, 0.4067,\n",
      "         0.1519, 0.1506, 0.9585, 0.7756, 0.8973, 0.4929, 0.2367, 0.8194, 0.4509,\n",
      "         0.2690, 0.8381]])\n",
      "torch.Size([20])\n",
      "tensor([0.1380, 0.0226, 0.3025, 0.1448, 0.5758, 0.8992, 0.2347, 0.1899, 0.4067,\n",
      "        0.1519, 0.1506, 0.9585, 0.7756, 0.8973, 0.4929, 0.2367, 0.8194, 0.4509,\n",
      "        0.2690, 0.8381])\n",
      "f torch.Size([2, 2])\n",
      "tensor([[0.8207, 0.6818],\n",
      "        [0.5057, 0.9335]])\n",
      "g torch.Size([2, 2])\n",
      "tensor([[[0.9769, 0.9769],\n",
      "         [0.2792, 0.2792],\n",
      "         [0.3277, 0.3277]],\n",
      "\n",
      "        [[0.9769, 0.9769],\n",
      "         [0.2792, 0.2792],\n",
      "         [0.3277, 0.3277]],\n",
      "\n",
      "        [[0.9769, 0.9769],\n",
      "         [0.2792, 0.2792],\n",
      "         [0.3277, 0.3277]],\n",
      "\n",
      "        [[0.9769, 0.9769],\n",
      "         [0.2792, 0.2792],\n",
      "         [0.3277, 0.3277]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(3, 226, 226)\n",
    "b = a.unsqueeze(0) # The unsqueeze() method adds a dimension of extent 1. unsqueeze(0) adds it as a new zeroth dimension - now you have a 1x3x226x226 tensor\n",
    "\n",
    "print(a.shape) # 3x226x226\n",
    "print(b.shape) # 1x3x226x226\n",
    "\n",
    "c = torch.rand(1, 1, 1, 1, 1) # 1x1x1x1x1 tensor\n",
    "print(c) # 1x1x1x1x1 tensor\n",
    "\n",
    "d = torch.rand(1, 20) # 1x20 tensor\n",
    "print(d.shape) # 1x20\n",
    "print(d)\n",
    "\n",
    "e = d.squeeze(0) # The squeeze() method removes dimensions of extent 1. squeeze(0) removes the zeroth dimension - now you have a 20-element tensor\n",
    "print(e.shape) # 1 x 20 -> \n",
    "print(e)\n",
    "\n",
    "f = torch.rand(2, 2) # 2x2 tensor\n",
    "print(\"f\", f.shape) # 2x2\n",
    "print(f)\n",
    "\n",
    "g = f.squeeze(0) # 2x2 tensor because there is no dimension of extent 1 to remove\n",
    "print(\"g\", g.shape)\n",
    "\n",
    "h = torch.ones(4, 3, 2)\n",
    "\n",
    "i = h * torch.rand(   3, 1) # 3rd dim = 1, 2nd dim identical to h\n",
    "print(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The squeeze() and unsqueeze() methods also have in-place versions, squeeze*() and unsqueeze*():\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radical Reshaping:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 20, 20])\n",
      "torch.Size([2400])\n",
      "torch.Size([2400])\n"
     ]
    }
   ],
   "source": [
    "output3d = torch.rand(6, 20, 20) # 6x20x20 tensor\n",
    "print(output3d.shape)\n",
    "\n",
    "input1d = output3d.reshape(6 * 20 * 20) # reshapes the tensor to a 1D tensor\n",
    "print(input1d.shape)\n",
    "\n",
    "# can also call reshape as a method on the torch module:\n",
    "print(torch.reshape(output3d, (6 * 20 * 20,)).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy Bridge\n",
    "\n",
    "### It’s easy to switch between ndarrays and PyTorch tensors:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "tensor([[0.7816, 0.9548, 0.9989],\n",
      "        [0.4472, 0.8589, 0.0920]])\n",
      "[[0.78155357 0.954831   0.9988768 ]\n",
      " [0.44722855 0.85890746 0.09204096]]\n"
     ]
    }
   ],
   "source": [
    "numpy_array = np.ones((2, 3))\n",
    "print(numpy_array)\n",
    "\n",
    "pytorch_tensor = torch.from_numpy(numpy_array) # convert numpy array to pytorch tensor\n",
    "print(pytorch_tensor)\n",
    "\n",
    "pytorch_rand = torch.rand(2, 3)\n",
    "print(pytorch_rand)\n",
    "\n",
    "numpy_rand = pytorch_rand.numpy() # convert pytorch tensor to numpy array\n",
    "print(numpy_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It is important to know that these converted objects are using the same underlying memory as their source objects, meaning that changes to one are reflected in the other:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  1.,  1.],\n",
      "        [ 1., 23.,  1.]], dtype=torch.float64)\n",
      "[[ 0.78155357  0.954831    0.9988768 ]\n",
      " [ 0.44722855 17.          0.09204096]]\n"
     ]
    }
   ],
   "source": [
    "numpy_array[1, 1] = 23\n",
    "print(pytorch_tensor) # pytorch tensor is a view of numpy array, so changing numpy array changes pytorch tensor\n",
    "\n",
    "pytorch_rand[1, 1] = 17\n",
    "print(numpy_rand) # numpy array is a view of pytorch tensor, so changing pytorch tensor changes numpy array"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
