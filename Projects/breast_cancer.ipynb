{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import opendatasets as od\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping, found downloaded files in \"./breast-cancer-wisconsin-data\" (use force=True to force download)\n"
     ]
    }
   ],
   "source": [
    "od.download(\"https://www.kaggle.com/uciml/breast-cancer-wisconsin-data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/breast-cancer-wisconsin-data/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0  ...          17.33           184.60      2019.0            0.1622   \n",
       "1  ...          23.41           158.80      1956.0            0.1238   \n",
       "2  ...          25.53           152.50      1709.0            0.1444   \n",
       "3  ...          26.50            98.87       567.7            0.2098   \n",
       "4  ...          16.67           152.20      1575.0            0.1374   \n",
       "\n",
       "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.6656           0.7119                0.2654          0.4601   \n",
       "1             0.1866           0.2416                0.1860          0.2750   \n",
       "2             0.4245           0.4504                0.2430          0.3613   \n",
       "3             0.8663           0.6869                0.2575          0.6638   \n",
       "4             0.2050           0.4000                0.1625          0.2364   \n",
       "\n",
       "   fractal_dimension_worst  Unnamed: 32  \n",
       "0                  0.11890          NaN  \n",
       "1                  0.08902          NaN  \n",
       "2                  0.08758          NaN  \n",
       "3                  0.17300          NaN  \n",
       "4                  0.07678          NaN  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                           0\n",
       "diagnosis                    0\n",
       "radius_mean                  0\n",
       "texture_mean                 0\n",
       "perimeter_mean               0\n",
       "area_mean                    0\n",
       "smoothness_mean              0\n",
       "compactness_mean             0\n",
       "concavity_mean               0\n",
       "concave points_mean          0\n",
       "symmetry_mean                0\n",
       "fractal_dimension_mean       0\n",
       "radius_se                    0\n",
       "texture_se                   0\n",
       "perimeter_se                 0\n",
       "area_se                      0\n",
       "smoothness_se                0\n",
       "compactness_se               0\n",
       "concavity_se                 0\n",
       "concave points_se            0\n",
       "symmetry_se                  0\n",
       "fractal_dimension_se         0\n",
       "radius_worst                 0\n",
       "texture_worst                0\n",
       "perimeter_worst              0\n",
       "area_worst                   0\n",
       "smoothness_worst             0\n",
       "compactness_worst            0\n",
       "concavity_worst              0\n",
       "concave points_worst         0\n",
       "symmetry_worst               0\n",
       "fractal_dimension_worst      0\n",
       "Unnamed: 32                569\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values in the dataset\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop any column that has all missing values and the id column as well\n",
    "data = data.drop([data.columns[-1], 'id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0         M        17.99         10.38          122.80     1001.0   \n",
       "1         M        20.57         17.77          132.90     1326.0   \n",
       "2         M        19.69         21.25          130.00     1203.0   \n",
       "3         M        11.42         20.38           77.58      386.1   \n",
       "4         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0         0.2419  ...         25.38          17.33           184.60   \n",
       "1         0.1812  ...         24.99          23.41           158.80   \n",
       "2         0.2069  ...         23.57          25.53           152.50   \n",
       "3         0.2597  ...         14.91          26.50            98.87   \n",
       "4         0.1809  ...         22.54          16.67           152.20   \n",
       "\n",
       "   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate target and features\n",
    "target = data['diagnosis']\n",
    "data = data.drop(['diagnosis'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,\n",
       "       0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,\n",
       "       0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,\n",
       "       1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,\n",
       "       1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert categorical 'diagnosis' column to numerical values\n",
    "target = target.map({'M': 1, 'B': 0}).astype(int) # 1 for malignant and 0 for benign\n",
    "\n",
    "target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training and testing sets, with 10% of the data used for testing.\n",
    "train_data, test_data, train_target, test_target = train_test_split(data, target, test_size=0.1, random_state=42)\n",
    "# train_data is used for training the model, train_target is the target variable(true outputs) for the training data\n",
    "# test_data is used for testing the model, test_target is the target variable for the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(     radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
       " 82        25.220         24.91          171.50     1878.0          0.10630   \n",
       " 39        13.480         20.82           88.40      559.2          0.10160   \n",
       " 271       11.290         13.04           72.23      388.0          0.09834   \n",
       " 79        12.860         18.00           83.19      506.3          0.09934   \n",
       " 2         19.690         21.25          130.00     1203.0          0.10960   \n",
       " ..           ...           ...             ...        ...              ...   \n",
       " 71         8.888         14.64           58.79      244.0          0.09783   \n",
       " 106       11.640         18.33           75.17      412.5          0.11420   \n",
       " 270       14.290         16.82           90.30      632.6          0.06429   \n",
       " 435       13.980         19.62           91.12      599.5          0.10600   \n",
       " 102       12.180         20.52           77.22      458.7          0.08013   \n",
       " \n",
       "      compactness_mean  concavity_mean  concave points_mean  symmetry_mean  \\\n",
       " 82            0.26650         0.33390              0.18450         0.1829   \n",
       " 39            0.12550         0.10630              0.05439         0.1720   \n",
       " 271           0.07608         0.03265              0.02755         0.1769   \n",
       " 79            0.09546         0.03889              0.02315         0.1718   \n",
       " 2             0.15990         0.19740              0.12790         0.2069   \n",
       " ..                ...             ...                  ...            ...   \n",
       " 71            0.15310         0.08606              0.02872         0.1902   \n",
       " 106           0.10170         0.07070              0.03485         0.1801   \n",
       " 270           0.02675         0.00725              0.00625         0.1508   \n",
       " 435           0.11330         0.11260              0.06463         0.1669   \n",
       " 102           0.04038         0.02383              0.01770         0.1739   \n",
       " \n",
       "      fractal_dimension_mean  ...  radius_worst  texture_worst  \\\n",
       " 82                  0.06782  ...        30.000          33.62   \n",
       " 39                  0.06419  ...        15.530          26.02   \n",
       " 271                 0.06270  ...        12.320          16.18   \n",
       " 79                  0.05997  ...        14.240          24.82   \n",
       " 2                   0.05999  ...        23.570          25.53   \n",
       " ..                      ...  ...           ...            ...   \n",
       " 71                  0.08980  ...         9.733          15.67   \n",
       " 106                 0.06520  ...        13.140          29.26   \n",
       " 270                 0.05376  ...        14.910          20.65   \n",
       " 435                 0.06544  ...        17.040          30.80   \n",
       " 102                 0.05677  ...        13.340          32.84   \n",
       " \n",
       "      perimeter_worst  area_worst  smoothness_worst  compactness_worst  \\\n",
       " 82            211.70      2562.0           0.15730            0.60760   \n",
       " 39            107.30       740.4           0.16100            0.42250   \n",
       " 271            78.27       457.5           0.13580            0.15070   \n",
       " 79             91.88       622.1           0.12890            0.21410   \n",
       " 2             152.50      1709.0           0.14440            0.42450   \n",
       " ..               ...         ...               ...                ...   \n",
       " 71             62.56       284.4           0.12070            0.24360   \n",
       " 106            85.51       521.7           0.16880            0.26600   \n",
       " 270            94.44       684.6           0.08567            0.05036   \n",
       " 435           113.90       869.3           0.16130            0.35680   \n",
       " 102            84.58       547.8           0.11230            0.08862   \n",
       " \n",
       "      concavity_worst  concave points_worst  symmetry_worst  \\\n",
       " 82           0.64760               0.28670          0.2355   \n",
       " 39           0.50300               0.22580          0.2807   \n",
       " 271          0.12750               0.08750          0.2733   \n",
       " 79           0.17310               0.07926          0.2779   \n",
       " 2            0.45040               0.24300          0.3613   \n",
       " ..               ...                   ...             ...   \n",
       " 71           0.14340               0.04786          0.2254   \n",
       " 106          0.28730               0.12180          0.2806   \n",
       " 270          0.03866               0.03333          0.2458   \n",
       " 435          0.40690               0.18270          0.3179   \n",
       " 102          0.11450               0.07431          0.2694   \n",
       " \n",
       "      fractal_dimension_worst  \n",
       " 82                   0.10510  \n",
       " 39                   0.10710  \n",
       " 271                  0.08022  \n",
       " 79                   0.07918  \n",
       " 2                    0.08758  \n",
       " ..                       ...  \n",
       " 71                   0.10840  \n",
       " 106                  0.09097  \n",
       " 270                  0.06120  \n",
       " 435                  0.10550  \n",
       " 102                  0.06878  \n",
       " \n",
       " [512 rows x 30 columns],\n",
       "      radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
       " 204       12.470         18.60           81.09      481.9          0.09965   \n",
       " 70        18.940         21.31          123.60     1130.0          0.09009   \n",
       " 131       15.460         19.48          101.70      748.9          0.10920   \n",
       " 431       12.400         17.68           81.47      467.8          0.10540   \n",
       " 540       11.540         14.44           74.65      402.9          0.09984   \n",
       " 567       20.600         29.33          140.10     1265.0          0.11780   \n",
       " 369       22.010         21.90          147.20     1482.0          0.10630   \n",
       " 29        17.570         15.05          115.00      955.1          0.09847   \n",
       " 81        13.340         15.86           86.49      520.0          0.10780   \n",
       " 477       13.900         16.62           88.97      599.4          0.06828   \n",
       " 457       13.210         25.25           84.10      537.9          0.08791   \n",
       " 167       16.780         18.80          109.30      886.3          0.08865   \n",
       " 165       14.970         19.76           95.50      690.2          0.08421   \n",
       " 329       16.260         21.88          107.50      826.8          0.11650   \n",
       " 527       12.340         12.27           78.94      468.5          0.09003   \n",
       " 83        19.100         26.29          129.10     1132.0          0.12150   \n",
       " 511       14.810         14.70           94.66      680.7          0.08472   \n",
       " 556       10.160         19.59           64.73      311.7          0.10030   \n",
       " 101        6.981         13.43           43.79      143.5          0.11700   \n",
       " 535       20.550         20.86          137.80     1308.0          0.10460   \n",
       " 73        13.800         15.79           90.43      584.1          0.10070   \n",
       " 394       12.100         17.72           78.07      446.2          0.10290   \n",
       " 393       21.610         22.28          144.40     1407.0          0.11670   \n",
       " 425       10.030         21.28           63.19      307.3          0.08117   \n",
       " 305       11.600         24.49           74.23      417.2          0.07474   \n",
       " 76        13.530         10.94           87.91      559.2          0.12910   \n",
       " 384       13.280         13.72           85.79      541.8          0.08363   \n",
       " 555       10.290         27.61           65.67      321.4          0.09030   \n",
       " 362       12.760         18.84           81.87      496.6          0.09676   \n",
       " 72        17.200         24.52          114.20      929.4          0.10710   \n",
       " 551       11.130         22.44           71.49      378.4          0.09566   \n",
       " 158       12.060         12.74           76.84      448.6          0.09311   \n",
       " 424        9.742         19.12           61.93      289.7          0.10750   \n",
       " 532       13.680         16.33           87.76      575.5          0.09277   \n",
       " 222       10.180         17.53           65.12      313.1          0.10610   \n",
       " 55        11.520         18.75           73.34      409.0          0.09524   \n",
       " 10        16.020         23.24          102.70      797.8          0.08206   \n",
       " 281       11.740         14.02           74.24      427.3          0.07813   \n",
       " 6         18.250         19.98          119.60     1040.0          0.09463   \n",
       " 90        14.620         24.02           94.57      662.7          0.08974   \n",
       " 104       10.490         19.29           67.41      336.1          0.09989   \n",
       " 353       15.080         25.74           98.00      716.6          0.10240   \n",
       " 422       11.610         16.02           75.46      408.2          0.10880   \n",
       " 211       11.840         18.94           75.51      428.0          0.08871   \n",
       " 275       11.890         17.36           76.20      435.6          0.12250   \n",
       " 109       11.340         21.26           72.48      396.5          0.08759   \n",
       " 520        9.295         13.90           59.96      257.8          0.13710   \n",
       " 557        9.423         27.88           59.26      271.3          0.08123   \n",
       " 531       11.670         20.02           75.21      416.2          0.10160   \n",
       " 284       12.890         15.70           84.08      516.6          0.07818   \n",
       " 264       17.190         22.07          111.60      928.3          0.09726   \n",
       " 30        18.630         25.11          124.80     1088.0          0.10640   \n",
       " 208       13.110         22.54           87.02      529.4          0.10020   \n",
       " 528       13.940         13.17           90.31      594.2          0.12480   \n",
       " 145       11.900         14.65           78.11      432.8          0.11520   \n",
       " 464       13.170         18.22           84.28      537.3          0.07466   \n",
       " 320       10.250         16.18           66.52      324.2          0.10610   \n",
       " \n",
       "      compactness_mean  concavity_mean  concave points_mean  symmetry_mean  \\\n",
       " 204           0.10580        0.080050             0.038210         0.1925   \n",
       " 70            0.10290        0.108000             0.079510         0.1582   \n",
       " 131           0.12230        0.146600             0.080870         0.1931   \n",
       " 431           0.13160        0.077410             0.027990         0.1811   \n",
       " 540           0.11200        0.067370             0.025940         0.1818   \n",
       " 567           0.27700        0.351400             0.152000         0.2397   \n",
       " 369           0.19540        0.244800             0.150100         0.1824   \n",
       " 29            0.11570        0.098750             0.079530         0.1739   \n",
       " 81            0.15350        0.116900             0.069870         0.1942   \n",
       " 477           0.05319        0.022240             0.013390         0.1813   \n",
       " 457           0.05205        0.027720             0.020680         0.1619   \n",
       " 167           0.09182        0.084220             0.065760         0.1893   \n",
       " 165           0.05352        0.019470             0.019390         0.1515   \n",
       " 329           0.12830        0.179900             0.079810         0.1869   \n",
       " 527           0.06307        0.029580             0.026470         0.1689   \n",
       " 83            0.17910        0.193700             0.146900         0.1634   \n",
       " 511           0.05016        0.034160             0.025410         0.1659   \n",
       " 556           0.07504        0.005025             0.011160         0.1791   \n",
       " 101           0.07568        0.000000             0.000000         0.1930   \n",
       " 535           0.17390        0.208500             0.132200         0.2127   \n",
       " 73            0.12800        0.077890             0.050690         0.1662   \n",
       " 394           0.09758        0.047830             0.033260         0.1937   \n",
       " 393           0.20870        0.281000             0.156200         0.2162   \n",
       " 425           0.03912        0.002470             0.005159         0.1630   \n",
       " 305           0.05688        0.019740             0.013130         0.1935   \n",
       " 76            0.10470        0.068770             0.065560         0.2403   \n",
       " 384           0.08575        0.050770             0.028640         0.1617   \n",
       " 555           0.07658        0.059990             0.027380         0.1593   \n",
       " 362           0.07952        0.026880             0.017810         0.1759   \n",
       " 72            0.18300        0.169200             0.079440         0.1927   \n",
       " 551           0.08194        0.048240             0.022570         0.2030   \n",
       " 158           0.05241        0.019720             0.019630         0.1590   \n",
       " 424           0.08333        0.008934             0.019670         0.2538   \n",
       " 532           0.07255        0.017520             0.018800         0.1631   \n",
       " 222           0.08502        0.017680             0.019150         0.1910   \n",
       " 55            0.05473        0.030360             0.022780         0.1920   \n",
       " 10            0.06669        0.032990             0.033230         0.1528   \n",
       " 281           0.04340        0.022450             0.027630         0.2101   \n",
       " 6             0.10900        0.112700             0.074000         0.1794   \n",
       " 90            0.08606        0.031020             0.029570         0.1685   \n",
       " 104           0.08578        0.029950             0.012010         0.2217   \n",
       " 353           0.09769        0.123500             0.065530         0.1647   \n",
       " 422           0.11680        0.070970             0.044970         0.1886   \n",
       " 211           0.06900        0.026690             0.013930         0.1533   \n",
       " 275           0.07210        0.059290             0.074040         0.2015   \n",
       " 109           0.06575        0.051330             0.018990         0.1487   \n",
       " 520           0.12250        0.033320             0.024210         0.2197   \n",
       " 557           0.04971        0.000000             0.000000         0.1742   \n",
       " 531           0.09453        0.042000             0.021570         0.1859   \n",
       " 284           0.09580        0.111500             0.033900         0.1432   \n",
       " 264           0.08995        0.090610             0.065270         0.1867   \n",
       " 30            0.18870        0.231900             0.124400         0.2183   \n",
       " 208           0.14830        0.087050             0.051020         0.1850   \n",
       " 528           0.09755        0.101000             0.066150         0.1976   \n",
       " 145           0.12960        0.037100             0.030030         0.1995   \n",
       " 464           0.05994        0.048590             0.028700         0.1454   \n",
       " 320           0.11110        0.067260             0.039650         0.1743   \n",
       " \n",
       "      fractal_dimension_mean  ...  radius_worst  texture_worst  \\\n",
       " 204                 0.06373  ...         14.97          24.64   \n",
       " 70                  0.05461  ...         24.86          26.58   \n",
       " 131                 0.05796  ...         19.26          26.00   \n",
       " 431                 0.07102  ...         12.88          22.91   \n",
       " 540                 0.06782  ...         12.26          19.68   \n",
       " 567                 0.07016  ...         25.74          39.42   \n",
       " 369                 0.06140  ...         27.66          25.80   \n",
       " 29                  0.06149  ...         20.01          19.52   \n",
       " 81                  0.06902  ...         15.53          23.19   \n",
       " 477                 0.05536  ...         15.14          21.80   \n",
       " 457                 0.05584  ...         14.35          34.23   \n",
       " 167                 0.05534  ...         20.05          26.30   \n",
       " 165                 0.05266  ...         15.98          25.82   \n",
       " 329                 0.06532  ...         17.73          25.21   \n",
       " 527                 0.05808  ...         13.61          19.27   \n",
       " 83                  0.07224  ...         20.33          32.72   \n",
       " 511                 0.05348  ...         15.61          17.58   \n",
       " 556                 0.06331  ...         10.65          22.88   \n",
       " 101                 0.07818  ...          7.93          19.54   \n",
       " 535                 0.06251  ...         24.30          25.48   \n",
       " 73                  0.06566  ...         16.57          20.86   \n",
       " 394                 0.06161  ...         13.56          25.80   \n",
       " 393                 0.06606  ...         26.23          28.74   \n",
       " 425                 0.06439  ...         11.11          28.94   \n",
       " 305                 0.05878  ...         12.44          31.62   \n",
       " 76                  0.06641  ...         14.08          12.49   \n",
       " 384                 0.05594  ...         14.24          17.37   \n",
       " 555                 0.06127  ...         10.84          34.91   \n",
       " 362                 0.06183  ...         13.75          25.99   \n",
       " 72                  0.06487  ...         23.32          33.82   \n",
       " 551                 0.06552  ...         12.02          28.26   \n",
       " 158                 0.05907  ...         13.14          18.41   \n",
       " 424                 0.07029  ...         11.21          23.17   \n",
       " 532                 0.06155  ...         15.85          20.20   \n",
       " 222                 0.06908  ...         11.17          22.84   \n",
       " 55                  0.05907  ...         12.84          22.47   \n",
       " 10                  0.05697  ...         19.19          33.88   \n",
       " 281                 0.06113  ...         13.31          18.26   \n",
       " 6                   0.05742  ...         22.88          27.66   \n",
       " 90                  0.05866  ...         16.11          29.11   \n",
       " 104                 0.06481  ...         11.54          23.31   \n",
       " 353                 0.06464  ...         18.51          33.22   \n",
       " 422                 0.06320  ...         12.64          19.67   \n",
       " 211                 0.06057  ...         13.30          24.99   \n",
       " 275                 0.05875  ...         12.40          18.99   \n",
       " 109                 0.06529  ...         13.01          29.15   \n",
       " 520                 0.07696  ...         10.57          17.84   \n",
       " 557                 0.06059  ...         10.49          34.24   \n",
       " 531                 0.06461  ...         13.35          28.81   \n",
       " 284                 0.05935  ...         13.90          19.69   \n",
       " 264                 0.05580  ...         21.58          29.33   \n",
       " 30                  0.06197  ...         23.15          34.01   \n",
       " 208                 0.07310  ...         14.55          29.16   \n",
       " 528                 0.06457  ...         14.62          15.38   \n",
       " 145                 0.07839  ...         13.15          16.51   \n",
       " 464                 0.05549  ...         14.90          23.89   \n",
       " 320                 0.07279  ...         11.28          20.61   \n",
       " \n",
       "      perimeter_worst  area_worst  smoothness_worst  compactness_worst  \\\n",
       " 204            96.05       677.9           0.14260            0.23780   \n",
       " 70            165.90      1866.0           0.11930            0.23360   \n",
       " 131           124.90      1156.0           0.15460            0.23940   \n",
       " 431            89.61       515.8           0.14500            0.26290   \n",
       " 540            78.78       457.8           0.13450            0.21180   \n",
       " 567           184.60      1821.0           0.16500            0.86810   \n",
       " 369           195.00      2227.0           0.12940            0.38850   \n",
       " 29            134.90      1227.0           0.12550            0.28120   \n",
       " 81             96.66       614.9           0.15360            0.47910   \n",
       " 477           101.20       718.9           0.09384            0.20060   \n",
       " 457            91.29       632.9           0.12890            0.10630   \n",
       " 167           130.70      1260.0           0.11680            0.21190   \n",
       " 165           102.30       782.1           0.10450            0.09995   \n",
       " 329           113.70       975.2           0.14260            0.21160   \n",
       " 527            87.22       564.9           0.12920            0.20740   \n",
       " 83            141.30      1298.0           0.13920            0.28170   \n",
       " 511           101.70       760.2           0.11390            0.10110   \n",
       " 556            67.88       347.3           0.12650            0.12000   \n",
       " 101            50.41       185.2           0.15840            0.12020   \n",
       " 535           160.20      1809.0           0.12680            0.31350   \n",
       " 73            110.30       812.4           0.14110            0.35420   \n",
       " 394            88.33       559.5           0.14320            0.17730   \n",
       " 393           172.00      2081.0           0.15020            0.57170   \n",
       " 425            69.92       376.3           0.11260            0.07094   \n",
       " 305            81.39       476.5           0.09545            0.13610   \n",
       " 76             91.36       605.5           0.14510            0.13790   \n",
       " 384            96.59       623.7           0.11660            0.26850   \n",
       " 555            69.57       357.6           0.13840            0.17100   \n",
       " 362            87.82       579.7           0.12980            0.18390   \n",
       " 72            151.60      1681.0           0.15850            0.73940   \n",
       " 551            77.80       436.6           0.10870            0.17820   \n",
       " 158            84.08       532.8           0.12750            0.12320   \n",
       " 424            71.79       380.9           0.13980            0.13520   \n",
       " 532           101.60       773.4           0.12640            0.15640   \n",
       " 222            71.94       375.6           0.14060            0.14400   \n",
       " 55             81.81       506.2           0.12490            0.08720   \n",
       " 10            123.80      1150.0           0.11810            0.15510   \n",
       " 281            84.70       533.7           0.10360            0.08500   \n",
       " 6             153.20      1606.0           0.14420            0.25760   \n",
       " 90            102.90       803.7           0.11150            0.17660   \n",
       " 104            74.22       402.8           0.12190            0.14860   \n",
       " 353           121.20      1050.0           0.16600            0.23560   \n",
       " 422            81.93       475.7           0.14150            0.21700   \n",
       " 211            85.22       546.3           0.12800            0.18800   \n",
       " 275            79.46       472.4           0.13590            0.08368   \n",
       " 109            83.99       518.1           0.16990            0.21960   \n",
       " 520            67.84       326.6           0.18500            0.20970   \n",
       " 557            66.50       330.6           0.10730            0.07158   \n",
       " 531            87.00       550.6           0.15500            0.29640   \n",
       " 284            92.12       595.6           0.09926            0.23170   \n",
       " 264           140.50      1436.0           0.15580            0.25670   \n",
       " 30            160.50      1670.0           0.14910            0.42570   \n",
       " 208            99.48       639.3           0.13490            0.44020   \n",
       " 528            94.52       653.3           0.13940            0.13640   \n",
       " 145            86.26       509.6           0.14240            0.25170   \n",
       " 464            95.10       687.6           0.12820            0.19650   \n",
       " 320            71.53       390.4           0.14020            0.23600   \n",
       " \n",
       "      concavity_worst  concave points_worst  symmetry_worst  \\\n",
       " 204          0.26710               0.10150          0.3014   \n",
       " 70           0.26870               0.17890          0.2551   \n",
       " 131          0.37910               0.15140          0.2837   \n",
       " 431          0.24030               0.07370          0.2556   \n",
       " 540          0.17970               0.06918          0.2329   \n",
       " 567          0.93870               0.26500          0.4087   \n",
       " 369          0.47560               0.24320          0.2741   \n",
       " 29           0.24890               0.14560          0.2756   \n",
       " 81           0.48580               0.17080          0.3527   \n",
       " 477          0.13840               0.06222          0.2679   \n",
       " 457          0.13900               0.06005          0.2444   \n",
       " 167          0.23180               0.14740          0.2810   \n",
       " 165          0.07750               0.05754          0.2646   \n",
       " 329          0.33440               0.10470          0.2736   \n",
       " 527          0.17910               0.10700          0.3110   \n",
       " 83           0.24320               0.18410          0.2311   \n",
       " 511          0.11010               0.07955          0.2334   \n",
       " 556          0.01005               0.02232          0.2262   \n",
       " 101          0.00000               0.00000          0.2932   \n",
       " 535          0.44330               0.21480          0.3077   \n",
       " 73           0.27790               0.13830          0.2589   \n",
       " 394          0.16030               0.06266          0.3049   \n",
       " 393          0.70530               0.24220          0.3828   \n",
       " 425          0.01235               0.02579          0.2349   \n",
       " 305          0.07239               0.04815          0.3244   \n",
       " 76           0.08539               0.07407          0.2710   \n",
       " 384          0.28660               0.09173          0.2736   \n",
       " 555          0.20000               0.09127          0.2226   \n",
       " 362          0.12550               0.08312          0.2744   \n",
       " 72           0.65660               0.18990          0.3313   \n",
       " 551          0.15640               0.06413          0.3169   \n",
       " 158          0.08636               0.07025          0.2514   \n",
       " 424          0.02085               0.04589          0.3196   \n",
       " 532          0.12060               0.08704          0.2806   \n",
       " 222          0.06572               0.05575          0.3055   \n",
       " 55           0.09076               0.06316          0.3306   \n",
       " 10           0.14590               0.09975          0.2948   \n",
       " 281          0.06735               0.08290          0.3101   \n",
       " 6            0.37840               0.19320          0.3063   \n",
       " 90           0.09189               0.06946          0.2522   \n",
       " 104          0.07987               0.03203          0.2826   \n",
       " 353          0.40290               0.15260          0.2654   \n",
       " 422          0.23020               0.11050          0.2787   \n",
       " 211          0.14710               0.06913          0.2535   \n",
       " 275          0.07153               0.08946          0.2220   \n",
       " 109          0.31200               0.08278          0.2829   \n",
       " 520          0.09996               0.07262          0.3681   \n",
       " 557          0.00000               0.00000          0.2475   \n",
       " 531          0.27580               0.08120          0.3206   \n",
       " 284          0.33440               0.10170          0.1999   \n",
       " 264          0.38890               0.19840          0.3216   \n",
       " 30           0.61330               0.18480          0.3444   \n",
       " 208          0.31620               0.11260          0.4128   \n",
       " 528          0.15590               0.10150          0.2160   \n",
       " 145          0.09420               0.06042          0.2727   \n",
       " 464          0.18760               0.10450          0.2235   \n",
       " 320          0.18980               0.09744          0.2608   \n",
       " \n",
       "      fractal_dimension_worst  \n",
       " 204                  0.08750  \n",
       " 70                   0.06589  \n",
       " 131                  0.08019  \n",
       " 431                  0.09359  \n",
       " 540                  0.08134  \n",
       " 567                  0.12400  \n",
       " 369                  0.08574  \n",
       " 29                   0.07919  \n",
       " 81                   0.10160  \n",
       " 477                  0.07698  \n",
       " 457                  0.06788  \n",
       " 167                  0.07228  \n",
       " 165                  0.06085  \n",
       " 329                  0.07953  \n",
       " 527                  0.07592  \n",
       " 83                   0.09203  \n",
       " 511                  0.06142  \n",
       " 556                  0.06742  \n",
       " 101                  0.09382  \n",
       " 535                  0.07569  \n",
       " 73                   0.10300  \n",
       " 394                  0.07081  \n",
       " 393                  0.10070  \n",
       " 425                  0.08061  \n",
       " 305                  0.06745  \n",
       " 76                   0.07191  \n",
       " 384                  0.07320  \n",
       " 555                  0.08283  \n",
       " 362                  0.07238  \n",
       " 72                   0.13390  \n",
       " 551                  0.08032  \n",
       " 158                  0.07898  \n",
       " 424                  0.08009  \n",
       " 532                  0.07782  \n",
       " 222                  0.08797  \n",
       " 55                   0.07036  \n",
       " 10                   0.08452  \n",
       " 281                  0.06688  \n",
       " 6                    0.08368  \n",
       " 90                   0.07246  \n",
       " 104                  0.07552  \n",
       " 353                  0.09438  \n",
       " 422                  0.07427  \n",
       " 211                  0.07993  \n",
       " 275                  0.06033  \n",
       " 109                  0.08832  \n",
       " 520                  0.08982  \n",
       " 557                  0.06969  \n",
       " 531                  0.08950  \n",
       " 284                  0.07127  \n",
       " 264                  0.07570  \n",
       " 30                   0.09782  \n",
       " 208                  0.10760  \n",
       " 528                  0.07253  \n",
       " 145                  0.10360  \n",
       " 464                  0.06925  \n",
       " 320                  0.09702  \n",
       " \n",
       " [57 rows x 30 columns])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((512, 30), (57, 30))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>14.127292</td>\n",
       "      <td>19.289649</td>\n",
       "      <td>91.969033</td>\n",
       "      <td>654.889104</td>\n",
       "      <td>0.096360</td>\n",
       "      <td>0.104341</td>\n",
       "      <td>0.088799</td>\n",
       "      <td>0.048919</td>\n",
       "      <td>0.181162</td>\n",
       "      <td>0.062798</td>\n",
       "      <td>...</td>\n",
       "      <td>16.269190</td>\n",
       "      <td>25.677223</td>\n",
       "      <td>107.261213</td>\n",
       "      <td>880.583128</td>\n",
       "      <td>0.132369</td>\n",
       "      <td>0.254265</td>\n",
       "      <td>0.272188</td>\n",
       "      <td>0.114606</td>\n",
       "      <td>0.290076</td>\n",
       "      <td>0.083946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.524049</td>\n",
       "      <td>4.301036</td>\n",
       "      <td>24.298981</td>\n",
       "      <td>351.914129</td>\n",
       "      <td>0.014064</td>\n",
       "      <td>0.052813</td>\n",
       "      <td>0.079720</td>\n",
       "      <td>0.038803</td>\n",
       "      <td>0.027414</td>\n",
       "      <td>0.007060</td>\n",
       "      <td>...</td>\n",
       "      <td>4.833242</td>\n",
       "      <td>6.146258</td>\n",
       "      <td>33.602542</td>\n",
       "      <td>569.356993</td>\n",
       "      <td>0.022832</td>\n",
       "      <td>0.157336</td>\n",
       "      <td>0.208624</td>\n",
       "      <td>0.065732</td>\n",
       "      <td>0.061867</td>\n",
       "      <td>0.018061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.981000</td>\n",
       "      <td>9.710000</td>\n",
       "      <td>43.790000</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>0.049960</td>\n",
       "      <td>...</td>\n",
       "      <td>7.930000</td>\n",
       "      <td>12.020000</td>\n",
       "      <td>50.410000</td>\n",
       "      <td>185.200000</td>\n",
       "      <td>0.071170</td>\n",
       "      <td>0.027290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156500</td>\n",
       "      <td>0.055040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>11.700000</td>\n",
       "      <td>16.170000</td>\n",
       "      <td>75.170000</td>\n",
       "      <td>420.300000</td>\n",
       "      <td>0.086370</td>\n",
       "      <td>0.064920</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>0.161900</td>\n",
       "      <td>0.057700</td>\n",
       "      <td>...</td>\n",
       "      <td>13.010000</td>\n",
       "      <td>21.080000</td>\n",
       "      <td>84.110000</td>\n",
       "      <td>515.300000</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.114500</td>\n",
       "      <td>0.064930</td>\n",
       "      <td>0.250400</td>\n",
       "      <td>0.071460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>13.370000</td>\n",
       "      <td>18.840000</td>\n",
       "      <td>86.240000</td>\n",
       "      <td>551.100000</td>\n",
       "      <td>0.095870</td>\n",
       "      <td>0.092630</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>...</td>\n",
       "      <td>14.970000</td>\n",
       "      <td>25.410000</td>\n",
       "      <td>97.660000</td>\n",
       "      <td>686.500000</td>\n",
       "      <td>0.131300</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>0.099930</td>\n",
       "      <td>0.282200</td>\n",
       "      <td>0.080040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>15.780000</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>104.100000</td>\n",
       "      <td>782.700000</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>0.066120</td>\n",
       "      <td>...</td>\n",
       "      <td>18.790000</td>\n",
       "      <td>29.720000</td>\n",
       "      <td>125.400000</td>\n",
       "      <td>1084.000000</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>0.382900</td>\n",
       "      <td>0.161400</td>\n",
       "      <td>0.317900</td>\n",
       "      <td>0.092080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>28.110000</td>\n",
       "      <td>39.280000</td>\n",
       "      <td>188.500000</td>\n",
       "      <td>2501.000000</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>0.345400</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>0.097440</td>\n",
       "      <td>...</td>\n",
       "      <td>36.040000</td>\n",
       "      <td>49.540000</td>\n",
       "      <td>251.200000</td>\n",
       "      <td>4254.000000</td>\n",
       "      <td>0.222600</td>\n",
       "      <td>1.058000</td>\n",
       "      <td>1.252000</td>\n",
       "      <td>0.291000</td>\n",
       "      <td>0.663800</td>\n",
       "      <td>0.207500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       radius_mean  texture_mean  perimeter_mean    area_mean  \\\n",
       "count   569.000000    569.000000      569.000000   569.000000   \n",
       "mean     14.127292     19.289649       91.969033   654.889104   \n",
       "std       3.524049      4.301036       24.298981   351.914129   \n",
       "min       6.981000      9.710000       43.790000   143.500000   \n",
       "25%      11.700000     16.170000       75.170000   420.300000   \n",
       "50%      13.370000     18.840000       86.240000   551.100000   \n",
       "75%      15.780000     21.800000      104.100000   782.700000   \n",
       "max      28.110000     39.280000      188.500000  2501.000000   \n",
       "\n",
       "       smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "count       569.000000        569.000000      569.000000           569.000000   \n",
       "mean          0.096360          0.104341        0.088799             0.048919   \n",
       "std           0.014064          0.052813        0.079720             0.038803   \n",
       "min           0.052630          0.019380        0.000000             0.000000   \n",
       "25%           0.086370          0.064920        0.029560             0.020310   \n",
       "50%           0.095870          0.092630        0.061540             0.033500   \n",
       "75%           0.105300          0.130400        0.130700             0.074000   \n",
       "max           0.163400          0.345400        0.426800             0.201200   \n",
       "\n",
       "       symmetry_mean  fractal_dimension_mean  ...  radius_worst  \\\n",
       "count     569.000000              569.000000  ...    569.000000   \n",
       "mean        0.181162                0.062798  ...     16.269190   \n",
       "std         0.027414                0.007060  ...      4.833242   \n",
       "min         0.106000                0.049960  ...      7.930000   \n",
       "25%         0.161900                0.057700  ...     13.010000   \n",
       "50%         0.179200                0.061540  ...     14.970000   \n",
       "75%         0.195700                0.066120  ...     18.790000   \n",
       "max         0.304000                0.097440  ...     36.040000   \n",
       "\n",
       "       texture_worst  perimeter_worst   area_worst  smoothness_worst  \\\n",
       "count     569.000000       569.000000   569.000000        569.000000   \n",
       "mean       25.677223       107.261213   880.583128          0.132369   \n",
       "std         6.146258        33.602542   569.356993          0.022832   \n",
       "min        12.020000        50.410000   185.200000          0.071170   \n",
       "25%        21.080000        84.110000   515.300000          0.116600   \n",
       "50%        25.410000        97.660000   686.500000          0.131300   \n",
       "75%        29.720000       125.400000  1084.000000          0.146000   \n",
       "max        49.540000       251.200000  4254.000000          0.222600   \n",
       "\n",
       "       compactness_worst  concavity_worst  concave points_worst  \\\n",
       "count         569.000000       569.000000            569.000000   \n",
       "mean            0.254265         0.272188              0.114606   \n",
       "std             0.157336         0.208624              0.065732   \n",
       "min             0.027290         0.000000              0.000000   \n",
       "25%             0.147200         0.114500              0.064930   \n",
       "50%             0.211900         0.226700              0.099930   \n",
       "75%             0.339100         0.382900              0.161400   \n",
       "max             1.058000         1.252000              0.291000   \n",
       "\n",
       "       symmetry_worst  fractal_dimension_worst  \n",
       "count      569.000000               569.000000  \n",
       "mean         0.290076                 0.083946  \n",
       "std          0.061867                 0.018061  \n",
       "min          0.156500                 0.055040  \n",
       "25%          0.250400                 0.071460  \n",
       "50%          0.282200                 0.080040  \n",
       "75%          0.317900                 0.092080  \n",
       "max          0.663800                 0.207500  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sca = StandardScaler()\n",
    "data = sca.fit_transform(train_data)\n",
    "\n",
    "# scales the features of the training data using standard scaling. This standardizes data by removing the mean and scaling to unit variance\n",
    "# This ensures that each feature has a mean of 0 and a standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The same scaling is applied to the testing data. This is important because the model has been trained on scaled data, so it must also be tested on scaled data.\n",
    "train_data = sca.transform(train_data)\n",
    "test_data = sca.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.121332</td>\n",
       "      <td>1.306752</td>\n",
       "      <td>3.248510</td>\n",
       "      <td>3.425702</td>\n",
       "      <td>0.733053</td>\n",
       "      <td>3.033049</td>\n",
       "      <td>3.042526</td>\n",
       "      <td>3.505980</td>\n",
       "      <td>0.074546</td>\n",
       "      <td>0.706655</td>\n",
       "      <td>...</td>\n",
       "      <td>2.818496</td>\n",
       "      <td>1.273621</td>\n",
       "      <td>3.089819</td>\n",
       "      <td>2.907350</td>\n",
       "      <td>1.082228</td>\n",
       "      <td>2.223136</td>\n",
       "      <td>1.763205</td>\n",
       "      <td>2.583719</td>\n",
       "      <td>-0.872533</td>\n",
       "      <td>1.139351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.195574</td>\n",
       "      <td>0.354820</td>\n",
       "      <td>-0.158917</td>\n",
       "      <td>-0.281746</td>\n",
       "      <td>0.396762</td>\n",
       "      <td>0.391127</td>\n",
       "      <td>0.203498</td>\n",
       "      <td>0.138340</td>\n",
       "      <td>-0.318672</td>\n",
       "      <td>0.199172</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.165183</td>\n",
       "      <td>0.042421</td>\n",
       "      <td>-0.012761</td>\n",
       "      <td>-0.256316</td>\n",
       "      <td>1.241824</td>\n",
       "      <td>1.050983</td>\n",
       "      <td>1.075407</td>\n",
       "      <td>1.663519</td>\n",
       "      <td>-0.158595</td>\n",
       "      <td>1.248131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.814315</td>\n",
       "      <td>-1.455946</td>\n",
       "      <td>-0.821950</td>\n",
       "      <td>-0.763028</td>\n",
       "      <td>0.163505</td>\n",
       "      <td>-0.534857</td>\n",
       "      <td>-0.715195</td>\n",
       "      <td>-0.556360</td>\n",
       "      <td>-0.141905</td>\n",
       "      <td>-0.009134</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.827078</td>\n",
       "      <td>-1.551660</td>\n",
       "      <td>-0.875481</td>\n",
       "      <td>-0.747643</td>\n",
       "      <td>0.154846</td>\n",
       "      <td>-0.670201</td>\n",
       "      <td>-0.710680</td>\n",
       "      <td>-0.426196</td>\n",
       "      <td>-0.275479</td>\n",
       "      <td>-0.213868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.370743</td>\n",
       "      <td>-0.301525</td>\n",
       "      <td>-0.372547</td>\n",
       "      <td>-0.430460</td>\n",
       "      <td>0.235056</td>\n",
       "      <td>-0.171733</td>\n",
       "      <td>-0.637359</td>\n",
       "      <td>-0.670246</td>\n",
       "      <td>-0.325887</td>\n",
       "      <td>-0.390794</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.431178</td>\n",
       "      <td>-0.151979</td>\n",
       "      <td>-0.471016</td>\n",
       "      <td>-0.461774</td>\n",
       "      <td>-0.142779</td>\n",
       "      <td>-0.268718</td>\n",
       "      <td>-0.493781</td>\n",
       "      <td>-0.550703</td>\n",
       "      <td>-0.202822</td>\n",
       "      <td>-0.270433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.558939</td>\n",
       "      <td>0.454901</td>\n",
       "      <td>1.546847</td>\n",
       "      <td>1.528123</td>\n",
       "      <td>0.969173</td>\n",
       "      <td>1.035681</td>\n",
       "      <td>1.339857</td>\n",
       "      <td>2.041001</td>\n",
       "      <td>0.940346</td>\n",
       "      <td>-0.387998</td>\n",
       "      <td>...</td>\n",
       "      <td>1.492646</td>\n",
       "      <td>-0.036959</td>\n",
       "      <td>1.330501</td>\n",
       "      <td>1.425901</td>\n",
       "      <td>0.525799</td>\n",
       "      <td>1.063648</td>\n",
       "      <td>0.825212</td>\n",
       "      <td>1.923411</td>\n",
       "      <td>1.114487</td>\n",
       "      <td>0.186441</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  3.121332  1.306752  3.248510  3.425702  0.733053  3.033049  3.042526   \n",
       "1 -0.195574  0.354820 -0.158917 -0.281746  0.396762  0.391127  0.203498   \n",
       "2 -0.814315 -1.455946 -0.821950 -0.763028  0.163505 -0.534857 -0.715195   \n",
       "3 -0.370743 -0.301525 -0.372547 -0.430460  0.235056 -0.171733 -0.637359   \n",
       "4  1.558939  0.454901  1.546847  1.528123  0.969173  1.035681  1.339857   \n",
       "\n",
       "         7         8         9   ...        20        21        22        23  \\\n",
       "0  3.505980  0.074546  0.706655  ...  2.818496  1.273621  3.089819  2.907350   \n",
       "1  0.138340 -0.318672  0.199172  ... -0.165183  0.042421 -0.012761 -0.256316   \n",
       "2 -0.556360 -0.141905 -0.009134  ... -0.827078 -1.551660 -0.875481 -0.747643   \n",
       "3 -0.670246 -0.325887 -0.390794  ... -0.431178 -0.151979 -0.471016 -0.461774   \n",
       "4  2.041001  0.940346 -0.387998  ...  1.492646 -0.036959  1.330501  1.425901   \n",
       "\n",
       "         24        25        26        27        28        29  \n",
       "0  1.082228  2.223136  1.763205  2.583719 -0.872533  1.139351  \n",
       "1  1.241824  1.050983  1.075407  1.663519 -0.158595  1.248131  \n",
       "2  0.154846 -0.670201 -0.710680 -0.426196 -0.275479 -0.213868  \n",
       "3 -0.142779 -0.268718 -0.493781 -0.550703 -0.202822 -0.270433  \n",
       "4  0.525799  1.063648  0.825212  1.923411  1.114487  0.186441  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(train_data).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the target to tensor\n",
    "train_data = torch.tensor(train_data, dtype=torch.float64)\n",
    "test_data = torch.tensor(test_data, dtype=torch.float64)\n",
    "train_target = torch.tensor(train_target.values, dtype=torch.float64)\n",
    "test_target = torch.tensor(test_target.values, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0.,\n",
       "        0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1.,\n",
       "        0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
       "        1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1.,\n",
       "        1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
       "        0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
       "        0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
       "        1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
       "        0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1.,\n",
       "        0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
       "        1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,\n",
       "        1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
       "        1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
       "        0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0.,\n",
       "        1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
       "        1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0.,\n",
       "        0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1.,\n",
       "        0., 1., 0., 0., 0., 0., 1., 0.], dtype=torch.float64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model\n",
    "class LRModel(torch.nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LRModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1) # input_dim is the number of features in the input data\n",
    "        self.sigmoid = nn.Sigmoid() # sigmoid activation function to convert the output to a probability\n",
    "\n",
    "    def forward(self, x):\n",
    "        output0 = self.linear(x)\n",
    "        output1 = self.sigmoid(output0) # apply the sigmoid activation function\n",
    "        return output1 # return the probability\n",
    "\n",
    "model = LRModel(train_data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.BCELoss() # Binary Cross Entropy Loss -> loss function for binary classification\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001) # SGD optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Training Loss: 0.7430297136306763, Test Loss: 0.7458170056343079\n",
      "Epoch: 20, Training Loss: 0.7193140983581543, Test Loss: 0.7237748503684998\n",
      "Epoch: 30, Training Loss: 0.6970258951187134, Test Loss: 0.7030025124549866\n",
      "Epoch: 40, Training Loss: 0.676088273525238, Test Loss: 0.6834333539009094\n",
      "Epoch: 50, Training Loss: 0.6564223170280457, Test Loss: 0.6649986505508423\n",
      "Epoch: 60, Training Loss: 0.637948751449585, Test Loss: 0.6476293206214905\n",
      "Epoch: 70, Training Loss: 0.6205889582633972, Test Loss: 0.6312577724456787\n",
      "Epoch: 80, Training Loss: 0.6042670011520386, Test Loss: 0.6158174872398376\n",
      "Epoch: 90, Training Loss: 0.588909924030304, Test Loss: 0.6012449860572815\n",
      "Epoch: 100, Training Loss: 0.5744486451148987, Test Loss: 0.5874800682067871\n",
      "Epoch: 110, Training Loss: 0.5608178377151489, Test Loss: 0.5744655132293701\n",
      "Epoch: 120, Training Loss: 0.5479569435119629, Test Loss: 0.5621480941772461\n",
      "Epoch: 130, Training Loss: 0.5358094573020935, Test Loss: 0.5504779815673828\n",
      "Epoch: 140, Training Loss: 0.5243232250213623, Test Loss: 0.5394090414047241\n",
      "Epoch: 150, Training Loss: 0.5134497880935669, Test Loss: 0.5288986563682556\n",
      "Epoch: 160, Training Loss: 0.5031449794769287, Test Loss: 0.518907368183136\n",
      "Epoch: 170, Training Loss: 0.49336785078048706, Test Loss: 0.5093988180160522\n",
      "Epoch: 180, Training Loss: 0.4840809404850006, Test Loss: 0.500339686870575\n",
      "Epoch: 190, Training Loss: 0.47524988651275635, Test Loss: 0.49169933795928955\n",
      "Epoch: 200, Training Loss: 0.4668431282043457, Test Loss: 0.4834495484828949\n",
      "Epoch: 210, Training Loss: 0.4588317275047302, Test Loss: 0.4755643308162689\n",
      "Epoch: 220, Training Loss: 0.4511890709400177, Test Loss: 0.4680198132991791\n",
      "Epoch: 230, Training Loss: 0.4438907504081726, Test Loss: 0.46079421043395996\n",
      "Epoch: 240, Training Loss: 0.4369143545627594, Test Loss: 0.4538672864437103\n",
      "Epoch: 250, Training Loss: 0.4302392899990082, Test Loss: 0.44722047448158264\n",
      "Epoch: 260, Training Loss: 0.423846572637558, Test Loss: 0.44083666801452637\n",
      "Epoch: 270, Training Loss: 0.41771870851516724, Test Loss: 0.43470004200935364\n",
      "Epoch: 280, Training Loss: 0.4118395149707794, Test Loss: 0.4287961423397064\n",
      "Epoch: 290, Training Loss: 0.40619412064552307, Test Loss: 0.42311134934425354\n",
      "Epoch: 300, Training Loss: 0.40076881647109985, Test Loss: 0.4176332652568817\n",
      "Epoch: 310, Training Loss: 0.39555084705352783, Test Loss: 0.4123503565788269\n",
      "Epoch: 320, Training Loss: 0.3905284106731415, Test Loss: 0.40725189447402954\n",
      "Epoch: 330, Training Loss: 0.3856905400753021, Test Loss: 0.4023279845714569\n",
      "Epoch: 340, Training Loss: 0.3810271918773651, Test Loss: 0.3975692093372345\n",
      "Epoch: 350, Training Loss: 0.376528799533844, Test Loss: 0.39296719431877136\n",
      "Epoch: 360, Training Loss: 0.37218666076660156, Test Loss: 0.3885137438774109\n",
      "Epoch: 370, Training Loss: 0.3679925501346588, Test Loss: 0.3842014968395233\n",
      "Epoch: 380, Training Loss: 0.36393892765045166, Test Loss: 0.38002344965934753\n",
      "Epoch: 390, Training Loss: 0.3600185513496399, Test Loss: 0.37597304582595825\n",
      "Epoch: 400, Training Loss: 0.3562248945236206, Test Loss: 0.37204423546791077\n",
      "Epoch: 410, Training Loss: 0.35255166888237, Test Loss: 0.3682312071323395\n",
      "Epoch: 420, Training Loss: 0.34899312257766724, Test Loss: 0.36452871561050415\n",
      "Epoch: 430, Training Loss: 0.34554368257522583, Test Loss: 0.36093172430992126\n",
      "Epoch: 440, Training Loss: 0.34219834208488464, Test Loss: 0.3574354350566864\n",
      "Epoch: 450, Training Loss: 0.3389522433280945, Test Loss: 0.35403549671173096\n",
      "Epoch: 460, Training Loss: 0.3358009159564972, Test Loss: 0.3507276475429535\n",
      "Epoch: 470, Training Loss: 0.3327401280403137, Test Loss: 0.3475080728530884\n",
      "Epoch: 480, Training Loss: 0.32976579666137695, Test Loss: 0.3443729281425476\n",
      "Epoch: 490, Training Loss: 0.3268742263317108, Test Loss: 0.3413187861442566\n",
      "Epoch: 500, Training Loss: 0.32406187057495117, Test Loss: 0.3383423388004303\n",
      "Epoch: 510, Training Loss: 0.3213253617286682, Test Loss: 0.335440456867218\n",
      "Epoch: 520, Training Loss: 0.31866157054901123, Test Loss: 0.33261018991470337\n",
      "Epoch: 530, Training Loss: 0.3160673975944519, Test Loss: 0.32984867691993713\n",
      "Epoch: 540, Training Loss: 0.31354019045829773, Test Loss: 0.32715341448783875\n",
      "Epoch: 550, Training Loss: 0.3110771179199219, Test Loss: 0.3245217800140381\n",
      "Epoch: 560, Training Loss: 0.30867573618888855, Test Loss: 0.32195141911506653\n",
      "Epoch: 570, Training Loss: 0.30633360147476196, Test Loss: 0.3194400668144226\n",
      "Epoch: 580, Training Loss: 0.30404847860336304, Test Loss: 0.316985547542572\n",
      "Epoch: 590, Training Loss: 0.30181822180747986, Test Loss: 0.3145858645439148\n",
      "Epoch: 600, Training Loss: 0.2996406555175781, Test Loss: 0.31223905086517334\n",
      "Epoch: 610, Training Loss: 0.2975139021873474, Test Loss: 0.3099433183670044\n",
      "Epoch: 620, Training Loss: 0.29543617367744446, Test Loss: 0.30769675970077515\n",
      "Epoch: 630, Training Loss: 0.29340559244155884, Test Loss: 0.30549782514572144\n",
      "Epoch: 640, Training Loss: 0.29142051935195923, Test Loss: 0.3033447861671448\n",
      "Epoch: 650, Training Loss: 0.2894793748855591, Test Loss: 0.3012363016605377\n",
      "Epoch: 660, Training Loss: 0.287580668926239, Test Loss: 0.2991707921028137\n",
      "Epoch: 670, Training Loss: 0.28572285175323486, Test Loss: 0.29714685678482056\n",
      "Epoch: 680, Training Loss: 0.2839045524597168, Test Loss: 0.29516321420669556\n",
      "Epoch: 690, Training Loss: 0.28212448954582214, Test Loss: 0.2932184934616089\n",
      "Epoch: 700, Training Loss: 0.28038138151168823, Test Loss: 0.2913115620613098\n",
      "Epoch: 710, Training Loss: 0.2786739766597748, Test Loss: 0.2894412577152252\n",
      "Epoch: 720, Training Loss: 0.27700117230415344, Test Loss: 0.28760644793510437\n",
      "Epoch: 730, Training Loss: 0.2753618359565735, Test Loss: 0.28580600023269653\n",
      "Epoch: 740, Training Loss: 0.27375492453575134, Test Loss: 0.2840389311313629\n",
      "Epoch: 750, Training Loss: 0.27217936515808105, Test Loss: 0.2823042571544647\n",
      "Epoch: 760, Training Loss: 0.270634263753891, Test Loss: 0.2806010842323303\n",
      "Epoch: 770, Training Loss: 0.26911860704421997, Test Loss: 0.27892839908599854\n",
      "Epoch: 780, Training Loss: 0.2676315903663635, Test Loss: 0.2772853970527649\n",
      "Epoch: 790, Training Loss: 0.26617231965065, Test Loss: 0.27567124366760254\n",
      "Epoch: 800, Training Loss: 0.26473990082740784, Test Loss: 0.2740851044654846\n",
      "Epoch: 810, Training Loss: 0.26333361864089966, Test Loss: 0.27252617478370667\n",
      "Epoch: 820, Training Loss: 0.2619527578353882, Test Loss: 0.27099379897117615\n",
      "Epoch: 830, Training Loss: 0.26059648394584656, Test Loss: 0.2694871425628662\n",
      "Epoch: 840, Training Loss: 0.25926414132118225, Test Loss: 0.2680056393146515\n",
      "Epoch: 850, Training Loss: 0.25795507431030273, Test Loss: 0.2665485143661499\n",
      "Epoch: 860, Training Loss: 0.2566685974597931, Test Loss: 0.2651151716709137\n",
      "Epoch: 870, Training Loss: 0.25540411472320557, Test Loss: 0.2637050151824951\n",
      "Epoch: 880, Training Loss: 0.2541610300540924, Test Loss: 0.262317419052124\n",
      "Epoch: 890, Training Loss: 0.25293871760368347, Test Loss: 0.26095184683799744\n",
      "Epoch: 900, Training Loss: 0.2517366409301758, Test Loss: 0.2596076428890228\n",
      "Epoch: 910, Training Loss: 0.25055432319641113, Test Loss: 0.25828438997268677\n",
      "Epoch: 920, Training Loss: 0.24939115345478058, Test Loss: 0.2569814920425415\n",
      "Epoch: 930, Training Loss: 0.2482466846704483, Test Loss: 0.2556985020637512\n",
      "Epoch: 940, Training Loss: 0.2471204400062561, Test Loss: 0.2544349133968353\n",
      "Epoch: 950, Training Loss: 0.24601192772388458, Test Loss: 0.25319021940231323\n",
      "Epoch: 960, Training Loss: 0.24492068588733673, Test Loss: 0.2519640326499939\n",
      "Epoch: 970, Training Loss: 0.24384631216526031, Test Loss: 0.2507558763027191\n",
      "Epoch: 980, Training Loss: 0.2427883893251419, Test Loss: 0.24956531822681427\n",
      "Epoch: 990, Training Loss: 0.2417464703321457, Test Loss: 0.24839197099208832\n",
      "Epoch: 1000, Training Loss: 0.24072019755840302, Test Loss: 0.24723544716835022\n",
      "Epoch: 1010, Training Loss: 0.23970919847488403, Test Loss: 0.24609537422657013\n",
      "Epoch: 1020, Training Loss: 0.2387130856513977, Test Loss: 0.24497130513191223\n",
      "Epoch: 1030, Training Loss: 0.23773154616355896, Test Loss: 0.24386295676231384\n",
      "Epoch: 1040, Training Loss: 0.23676416277885437, Test Loss: 0.24276992678642273\n",
      "Epoch: 1050, Training Loss: 0.23581068217754364, Test Loss: 0.2416919469833374\n",
      "Epoch: 1060, Training Loss: 0.23487073183059692, Test Loss: 0.24062861502170563\n",
      "Epoch: 1070, Training Loss: 0.23394401371479034, Test Loss: 0.2395796924829483\n",
      "Epoch: 1080, Training Loss: 0.23303022980690002, Test Loss: 0.238544762134552\n",
      "Epoch: 1090, Training Loss: 0.2321290522813797, Test Loss: 0.23752361536026\n",
      "Epoch: 1100, Training Loss: 0.23124030232429504, Test Loss: 0.23651590943336487\n",
      "Epoch: 1110, Training Loss: 0.23036359250545502, Test Loss: 0.23552142083644867\n",
      "Epoch: 1120, Training Loss: 0.22949868440628052, Test Loss: 0.23453979194164276\n",
      "Epoch: 1130, Training Loss: 0.22864536941051483, Test Loss: 0.23357082903385162\n",
      "Epoch: 1140, Training Loss: 0.2278033345937729, Test Loss: 0.23261424899101257\n",
      "Epoch: 1150, Training Loss: 0.22697237133979797, Test Loss: 0.23166975378990173\n",
      "Epoch: 1160, Training Loss: 0.22615227103233337, Test Loss: 0.23073717951774597\n",
      "Epoch: 1170, Training Loss: 0.22534272074699402, Test Loss: 0.2298162430524826\n",
      "Epoch: 1180, Training Loss: 0.22454358637332916, Test Loss: 0.22890672087669373\n",
      "Epoch: 1190, Training Loss: 0.22375458478927612, Test Loss: 0.22800835967063904\n",
      "Epoch: 1200, Training Loss: 0.22297552227973938, Test Loss: 0.22712096571922302\n",
      "Epoch: 1210, Training Loss: 0.2222062200307846, Test Loss: 0.22624437510967255\n",
      "Epoch: 1220, Training Loss: 0.22144649922847748, Test Loss: 0.22537828981876373\n",
      "Epoch: 1230, Training Loss: 0.2206960767507553, Test Loss: 0.22452257573604584\n",
      "Epoch: 1240, Training Loss: 0.21995486319065094, Test Loss: 0.22367699444293976\n",
      "Epoch: 1250, Training Loss: 0.21922260522842407, Test Loss: 0.22284136712551117\n",
      "Epoch: 1260, Training Loss: 0.21849916875362396, Test Loss: 0.22201552987098694\n",
      "Epoch: 1270, Training Loss: 0.21778437495231628, Test Loss: 0.22119927406311035\n",
      "Epoch: 1280, Training Loss: 0.2170780450105667, Test Loss: 0.22039242088794708\n",
      "Epoch: 1290, Training Loss: 0.2163800299167633, Test Loss: 0.2195948362350464\n",
      "Epoch: 1300, Training Loss: 0.21569013595581055, Test Loss: 0.21880629658699036\n",
      "Epoch: 1310, Training Loss: 0.21500824391841888, Test Loss: 0.21802665293216705\n",
      "Epoch: 1320, Training Loss: 0.214334174990654, Test Loss: 0.21725581586360931\n",
      "Epoch: 1330, Training Loss: 0.21366779506206512, Test Loss: 0.21649348735809326\n",
      "Epoch: 1340, Training Loss: 0.21300898492336273, Test Loss: 0.2157396376132965\n",
      "Epoch: 1350, Training Loss: 0.2123575657606125, Test Loss: 0.21499410271644592\n",
      "Epoch: 1360, Training Loss: 0.21171341836452484, Test Loss: 0.2142566591501236\n",
      "Epoch: 1370, Training Loss: 0.21107643842697144, Test Loss: 0.21352727711200714\n",
      "Epoch: 1380, Training Loss: 0.21044646203517914, Test Loss: 0.21280574798583984\n",
      "Epoch: 1390, Training Loss: 0.209823340177536, Test Loss: 0.21209193766117096\n",
      "Epoch: 1400, Training Loss: 0.2092069834470749, Test Loss: 0.21138572692871094\n",
      "Epoch: 1410, Training Loss: 0.2085973024368286, Test Loss: 0.21068699657917023\n",
      "Epoch: 1420, Training Loss: 0.20799411833286285, Test Loss: 0.20999564230442047\n",
      "Epoch: 1430, Training Loss: 0.20739735662937164, Test Loss: 0.20931147038936615\n",
      "Epoch: 1440, Training Loss: 0.20680686831474304, Test Loss: 0.2086344063282013\n",
      "Epoch: 1450, Training Loss: 0.20622259378433228, Test Loss: 0.20796434581279755\n",
      "Epoch: 1460, Training Loss: 0.2056443840265274, Test Loss: 0.20730115473270416\n",
      "Epoch: 1470, Training Loss: 0.20507217943668365, Test Loss: 0.2066447138786316\n",
      "Epoch: 1480, Training Loss: 0.2045058310031891, Test Loss: 0.20599490404129028\n",
      "Epoch: 1490, Training Loss: 0.20394527912139893, Test Loss: 0.20535168051719666\n",
      "Epoch: 1500, Training Loss: 0.20339040458202362, Test Loss: 0.20471487939357758\n",
      "Epoch: 1510, Training Loss: 0.202841117978096, Test Loss: 0.2040843963623047\n",
      "Epoch: 1520, Training Loss: 0.20229731500148773, Test Loss: 0.2034601867198944\n",
      "Epoch: 1530, Training Loss: 0.2017589509487152, Test Loss: 0.202842116355896\n",
      "Epoch: 1540, Training Loss: 0.2012258768081665, Test Loss: 0.2022300660610199\n",
      "Epoch: 1550, Training Loss: 0.20069804787635803, Test Loss: 0.20162397623062134\n",
      "Epoch: 1560, Training Loss: 0.20017535984516144, Test Loss: 0.20102375745773315\n",
      "Epoch: 1570, Training Loss: 0.19965775310993195, Test Loss: 0.20042935013771057\n",
      "Epoch: 1580, Training Loss: 0.1991450935602188, Test Loss: 0.19984054565429688\n",
      "Epoch: 1590, Training Loss: 0.19863736629486084, Test Loss: 0.19925734400749207\n",
      "Epoch: 1600, Training Loss: 0.19813445210456848, Test Loss: 0.19867967069149017\n",
      "Epoch: 1610, Training Loss: 0.19763633608818054, Test Loss: 0.19810745120048523\n",
      "Epoch: 1620, Training Loss: 0.1971428543329239, Test Loss: 0.1975405514240265\n",
      "Epoch: 1630, Training Loss: 0.19665399193763733, Test Loss: 0.19697892665863037\n",
      "Epoch: 1640, Training Loss: 0.1961696743965149, Test Loss: 0.1964224874973297\n",
      "Epoch: 1650, Training Loss: 0.19568981230258942, Test Loss: 0.19587115943431854\n",
      "Epoch: 1660, Training Loss: 0.19521436095237732, Test Loss: 0.1953248828649521\n",
      "Epoch: 1670, Training Loss: 0.19474323093891144, Test Loss: 0.19478356838226318\n",
      "Epoch: 1680, Training Loss: 0.19427639245986938, Test Loss: 0.19424714148044586\n",
      "Epoch: 1690, Training Loss: 0.1938137412071228, Test Loss: 0.19371554255485535\n",
      "Epoch: 1700, Training Loss: 0.1933552473783493, Test Loss: 0.19318872690200806\n",
      "Epoch: 1710, Training Loss: 0.19290083646774292, Test Loss: 0.19266657531261444\n",
      "Epoch: 1720, Training Loss: 0.19245043396949768, Test Loss: 0.1921490728855133\n",
      "Epoch: 1730, Training Loss: 0.19200399518013, Test Loss: 0.1916361004114151\n",
      "Epoch: 1740, Training Loss: 0.19156146049499512, Test Loss: 0.19112761318683624\n",
      "Epoch: 1750, Training Loss: 0.19112278521060944, Test Loss: 0.19062358140945435\n",
      "Epoch: 1760, Training Loss: 0.19068792462348938, Test Loss: 0.19012394547462463\n",
      "Epoch: 1770, Training Loss: 0.1902567744255066, Test Loss: 0.18962860107421875\n",
      "Epoch: 1780, Training Loss: 0.18982931971549988, Test Loss: 0.18913747370243073\n",
      "Epoch: 1790, Training Loss: 0.18940551578998566, Test Loss: 0.18865057826042175\n",
      "Epoch: 1800, Training Loss: 0.18898528814315796, Test Loss: 0.18816782534122467\n",
      "Epoch: 1810, Training Loss: 0.1885685920715332, Test Loss: 0.1876891553401947\n",
      "Epoch: 1820, Training Loss: 0.1881553828716278, Test Loss: 0.18721447885036469\n",
      "Epoch: 1830, Training Loss: 0.1877456158399582, Test Loss: 0.186743825674057\n",
      "Epoch: 1840, Training Loss: 0.18733927607536316, Test Loss: 0.18627707660198212\n",
      "Epoch: 1850, Training Loss: 0.18693622946739197, Test Loss: 0.18581418693065643\n",
      "Epoch: 1860, Training Loss: 0.186536505818367, Test Loss: 0.18535512685775757\n",
      "Epoch: 1870, Training Loss: 0.1861400306224823, Test Loss: 0.18489985167980194\n",
      "Epoch: 1880, Training Loss: 0.18574678897857666, Test Loss: 0.1844482719898224\n",
      "Epoch: 1890, Training Loss: 0.18535667657852173, Test Loss: 0.18400037288665771\n",
      "Epoch: 1900, Training Loss: 0.1849697232246399, Test Loss: 0.18355609476566315\n",
      "Epoch: 1910, Training Loss: 0.18458586931228638, Test Loss: 0.1831153929233551\n",
      "Epoch: 1920, Training Loss: 0.18420502543449402, Test Loss: 0.1826782375574112\n",
      "Epoch: 1930, Training Loss: 0.18382719159126282, Test Loss: 0.18224456906318665\n",
      "Epoch: 1940, Training Loss: 0.18345233798027039, Test Loss: 0.18181434273719788\n",
      "Epoch: 1950, Training Loss: 0.18308040499687195, Test Loss: 0.1813875138759613\n",
      "Epoch: 1960, Training Loss: 0.1827113777399063, Test Loss: 0.18096405267715454\n",
      "Epoch: 1970, Training Loss: 0.1823451817035675, Test Loss: 0.180543914437294\n",
      "Epoch: 1980, Training Loss: 0.18198181688785553, Test Loss: 0.18012703955173492\n",
      "Epoch: 1990, Training Loss: 0.1816212236881256, Test Loss: 0.1797133982181549\n",
      "Epoch: 2000, Training Loss: 0.18126340210437775, Test Loss: 0.17930299043655396\n",
      "Epoch: 2010, Training Loss: 0.18090826272964478, Test Loss: 0.17889569699764252\n",
      "Epoch: 2020, Training Loss: 0.1805558204650879, Test Loss: 0.17849154770374298\n",
      "Epoch: 2030, Training Loss: 0.1802060306072235, Test Loss: 0.17809049785137177\n",
      "Epoch: 2040, Training Loss: 0.17985883355140686, Test Loss: 0.1776924431324005\n",
      "Epoch: 2050, Training Loss: 0.17951421439647675, Test Loss: 0.177297443151474\n",
      "Epoch: 2060, Training Loss: 0.17917215824127197, Test Loss: 0.17690539360046387\n",
      "Epoch: 2070, Training Loss: 0.17883262038230896, Test Loss: 0.17651629447937012\n",
      "Epoch: 2080, Training Loss: 0.17849555611610413, Test Loss: 0.17613005638122559\n",
      "Epoch: 2090, Training Loss: 0.17816093564033508, Test Loss: 0.17574673891067505\n",
      "Epoch: 2100, Training Loss: 0.17782878875732422, Test Loss: 0.17536623775959015\n",
      "Epoch: 2110, Training Loss: 0.17749901115894318, Test Loss: 0.1749885231256485\n",
      "Epoch: 2120, Training Loss: 0.17717160284519196, Test Loss: 0.1746135950088501\n",
      "Epoch: 2130, Training Loss: 0.17684653401374817, Test Loss: 0.17424137890338898\n",
      "Epoch: 2140, Training Loss: 0.17652377486228943, Test Loss: 0.17387187480926514\n",
      "Epoch: 2150, Training Loss: 0.17620332539081573, Test Loss: 0.17350506782531738\n",
      "Epoch: 2160, Training Loss: 0.1758851259946823, Test Loss: 0.17314089834690094\n",
      "Epoch: 2170, Training Loss: 0.17556917667388916, Test Loss: 0.17277933657169342\n",
      "Epoch: 2180, Training Loss: 0.1752554178237915, Test Loss: 0.17242036759853363\n",
      "Epoch: 2190, Training Loss: 0.17494381964206696, Test Loss: 0.17206396162509918\n",
      "Epoch: 2200, Training Loss: 0.1746343970298767, Test Loss: 0.1717100590467453\n",
      "Epoch: 2210, Training Loss: 0.17432710528373718, Test Loss: 0.1713586449623108\n",
      "Epoch: 2220, Training Loss: 0.17402192950248718, Test Loss: 0.17100974917411804\n",
      "Epoch: 2230, Training Loss: 0.17371883988380432, Test Loss: 0.1706632524728775\n",
      "Epoch: 2240, Training Loss: 0.1734178215265274, Test Loss: 0.17031919956207275\n",
      "Epoch: 2250, Training Loss: 0.17311881482601166, Test Loss: 0.16997750103473663\n",
      "Epoch: 2260, Training Loss: 0.17282183468341827, Test Loss: 0.16963820159435272\n",
      "Epoch: 2270, Training Loss: 0.17252685129642487, Test Loss: 0.16930122673511505\n",
      "Epoch: 2280, Training Loss: 0.17223381996154785, Test Loss: 0.16896657645702362\n",
      "Epoch: 2290, Training Loss: 0.1719427853822708, Test Loss: 0.16863422095775604\n",
      "Epoch: 2300, Training Loss: 0.1716536432504654, Test Loss: 0.16830408573150635\n",
      "Epoch: 2310, Training Loss: 0.1713663935661316, Test Loss: 0.16797621548175812\n",
      "Epoch: 2320, Training Loss: 0.1710810661315918, Test Loss: 0.16765058040618896\n",
      "Epoch: 2330, Training Loss: 0.17079761624336243, Test Loss: 0.16732709109783173\n",
      "Epoch: 2340, Training Loss: 0.17051595449447632, Test Loss: 0.16700580716133118\n",
      "Epoch: 2350, Training Loss: 0.17023615539073944, Test Loss: 0.16668665409088135\n",
      "Epoch: 2360, Training Loss: 0.16995815932750702, Test Loss: 0.16636966168880463\n",
      "Epoch: 2370, Training Loss: 0.16968193650245667, Test Loss: 0.16605471074581146\n",
      "Epoch: 2380, Training Loss: 0.16940748691558838, Test Loss: 0.16574189066886902\n",
      "Epoch: 2390, Training Loss: 0.16913481056690216, Test Loss: 0.16543108224868774\n",
      "Epoch: 2400, Training Loss: 0.16886381804943085, Test Loss: 0.16512234508991241\n",
      "Epoch: 2410, Training Loss: 0.1685945838689804, Test Loss: 0.16481558978557587\n",
      "Epoch: 2420, Training Loss: 0.16832701861858368, Test Loss: 0.1645108461380005\n",
      "Epoch: 2430, Training Loss: 0.16806115210056305, Test Loss: 0.1642080545425415\n",
      "Epoch: 2440, Training Loss: 0.16779692471027374, Test Loss: 0.1639072448015213\n",
      "Epoch: 2450, Training Loss: 0.16753435134887695, Test Loss: 0.1636083871126175\n",
      "Epoch: 2460, Training Loss: 0.1672734022140503, Test Loss: 0.1633114069700241\n",
      "Epoch: 2470, Training Loss: 0.16701403260231018, Test Loss: 0.16301633417606354\n",
      "Epoch: 2480, Training Loss: 0.1667563021183014, Test Loss: 0.162723109126091\n",
      "Epoch: 2490, Training Loss: 0.16650012135505676, Test Loss: 0.1624317765235901\n",
      "Epoch: 2500, Training Loss: 0.1662455052137375, Test Loss: 0.16214226186275482\n",
      "Epoch: 2510, Training Loss: 0.16599245369434357, Test Loss: 0.1618545949459076\n",
      "Epoch: 2520, Training Loss: 0.16574090719223022, Test Loss: 0.16156870126724243\n",
      "Epoch: 2530, Training Loss: 0.16549088060855865, Test Loss: 0.16128459572792053\n",
      "Epoch: 2540, Training Loss: 0.16524237394332886, Test Loss: 0.1610022932291031\n",
      "Epoch: 2550, Training Loss: 0.16499531269073486, Test Loss: 0.16072168946266174\n",
      "Epoch: 2560, Training Loss: 0.16474974155426025, Test Loss: 0.16044287383556366\n",
      "Epoch: 2570, Training Loss: 0.16450561583042145, Test Loss: 0.1601657271385193\n",
      "Epoch: 2580, Training Loss: 0.16426296532154083, Test Loss: 0.159890279173851\n",
      "Epoch: 2590, Training Loss: 0.16402173042297363, Test Loss: 0.15961652994155884\n",
      "Epoch: 2600, Training Loss: 0.16378188133239746, Test Loss: 0.15934444963932037\n",
      "Epoch: 2610, Training Loss: 0.1635434329509735, Test Loss: 0.15907402336597443\n",
      "Epoch: 2620, Training Loss: 0.16330638527870178, Test Loss: 0.158805251121521\n",
      "Epoch: 2630, Training Loss: 0.1630707085132599, Test Loss: 0.15853804349899292\n",
      "Epoch: 2640, Training Loss: 0.16283640265464783, Test Loss: 0.15827247500419617\n",
      "Epoch: 2650, Training Loss: 0.16260342299938202, Test Loss: 0.15800851583480835\n",
      "Epoch: 2660, Training Loss: 0.16237178444862366, Test Loss: 0.1577461212873459\n",
      "Epoch: 2670, Training Loss: 0.16214145720005035, Test Loss: 0.1574852615594864\n",
      "Epoch: 2680, Training Loss: 0.1619124561548233, Test Loss: 0.15722596645355225\n",
      "Epoch: 2690, Training Loss: 0.16168472170829773, Test Loss: 0.1569681465625763\n",
      "Epoch: 2700, Training Loss: 0.1614583134651184, Test Loss: 0.1567119061946869\n",
      "Epoch: 2710, Training Loss: 0.16123317182064056, Test Loss: 0.15645714104175568\n",
      "Epoch: 2720, Training Loss: 0.1610092669725418, Test Loss: 0.15620392560958862\n",
      "Epoch: 2730, Training Loss: 0.16078661382198334, Test Loss: 0.15595212578773499\n",
      "Epoch: 2740, Training Loss: 0.16056522727012634, Test Loss: 0.15570177137851715\n",
      "Epoch: 2750, Training Loss: 0.16034503281116486, Test Loss: 0.1554528772830963\n",
      "Epoch: 2760, Training Loss: 0.16012607514858246, Test Loss: 0.15520545840263367\n",
      "Epoch: 2770, Training Loss: 0.15990830957889557, Test Loss: 0.15495942533016205\n",
      "Epoch: 2780, Training Loss: 0.1596917361021042, Test Loss: 0.15471482276916504\n",
      "Epoch: 2790, Training Loss: 0.1594763547182083, Test Loss: 0.15447159111499786\n",
      "Epoch: 2800, Training Loss: 0.15926213562488556, Test Loss: 0.1542297601699829\n",
      "Epoch: 2810, Training Loss: 0.15904907882213593, Test Loss: 0.1539893001317978\n",
      "Epoch: 2820, Training Loss: 0.15883716940879822, Test Loss: 0.1537501960992813\n",
      "Epoch: 2830, Training Loss: 0.15862639248371124, Test Loss: 0.15351244807243347\n",
      "Epoch: 2840, Training Loss: 0.1584167629480362, Test Loss: 0.15327602624893188\n",
      "Epoch: 2850, Training Loss: 0.1582082360982895, Test Loss: 0.15304093062877655\n",
      "Epoch: 2860, Training Loss: 0.15800084173679352, Test Loss: 0.15280717611312866\n",
      "Epoch: 2870, Training Loss: 0.1577945053577423, Test Loss: 0.15257465839385986\n",
      "Epoch: 2880, Training Loss: 0.15758930146694183, Test Loss: 0.1523434817790985\n",
      "Epoch: 2890, Training Loss: 0.15738517045974731, Test Loss: 0.15211355686187744\n",
      "Epoch: 2900, Training Loss: 0.15718209743499756, Test Loss: 0.15188491344451904\n",
      "Epoch: 2910, Training Loss: 0.15698009729385376, Test Loss: 0.15165750682353973\n",
      "Epoch: 2920, Training Loss: 0.15677915513515472, Test Loss: 0.1514313668012619\n",
      "Epoch: 2930, Training Loss: 0.15657924115657806, Test Loss: 0.15120646357536316\n",
      "Epoch: 2940, Training Loss: 0.15638037025928497, Test Loss: 0.15098276734352112\n",
      "Epoch: 2950, Training Loss: 0.15618252754211426, Test Loss: 0.15076029300689697\n",
      "Epoch: 2960, Training Loss: 0.15598569810390472, Test Loss: 0.15053902566432953\n",
      "Epoch: 2970, Training Loss: 0.15578985214233398, Test Loss: 0.1503189504146576\n",
      "Epoch: 2980, Training Loss: 0.15559504926204681, Test Loss: 0.15010003745555878\n",
      "Epoch: 2990, Training Loss: 0.15540122985839844, Test Loss: 0.14988230168819427\n",
      "Epoch: 3000, Training Loss: 0.15520837903022766, Test Loss: 0.1496657431125641\n",
      "Epoch: 3010, Training Loss: 0.15501649677753448, Test Loss: 0.14945034682750702\n",
      "Epoch: 3020, Training Loss: 0.1548256129026413, Test Loss: 0.14923608303070068\n",
      "Epoch: 3030, Training Loss: 0.15463565289974213, Test Loss: 0.14902296662330627\n",
      "Epoch: 3040, Training Loss: 0.15444667637348175, Test Loss: 0.1488109529018402\n",
      "Epoch: 3050, Training Loss: 0.1542586237192154, Test Loss: 0.14860005676746368\n",
      "Epoch: 3060, Training Loss: 0.15407149493694305, Test Loss: 0.1483902633190155\n",
      "Epoch: 3070, Training Loss: 0.15388533473014832, Test Loss: 0.14818161725997925\n",
      "Epoch: 3080, Training Loss: 0.153700053691864, Test Loss: 0.14797399938106537\n",
      "Epoch: 3090, Training Loss: 0.15351569652557373, Test Loss: 0.14776748418807983\n",
      "Epoch: 3100, Training Loss: 0.15333224833011627, Test Loss: 0.14756202697753906\n",
      "Epoch: 3110, Training Loss: 0.15314970910549164, Test Loss: 0.14735765755176544\n",
      "Epoch: 3120, Training Loss: 0.15296803414821625, Test Loss: 0.147154301404953\n",
      "Epoch: 3130, Training Loss: 0.1527872532606125, Test Loss: 0.1469520479440689\n",
      "Epoch: 3140, Training Loss: 0.15260735154151917, Test Loss: 0.1467507779598236\n",
      "Epoch: 3150, Training Loss: 0.15242831408977509, Test Loss: 0.14655056595802307\n",
      "Epoch: 3160, Training Loss: 0.15225014090538025, Test Loss: 0.14635135233402252\n",
      "Epoch: 3170, Training Loss: 0.15207283198833466, Test Loss: 0.14615316689014435\n",
      "Epoch: 3180, Training Loss: 0.15189635753631592, Test Loss: 0.14595597982406616\n",
      "Epoch: 3190, Training Loss: 0.15172074735164642, Test Loss: 0.14575977623462677\n",
      "Epoch: 3200, Training Loss: 0.1515459418296814, Test Loss: 0.14556460082530975\n",
      "Epoch: 3210, Training Loss: 0.15137198567390442, Test Loss: 0.14537036418914795\n",
      "Epoch: 3220, Training Loss: 0.1511988341808319, Test Loss: 0.14517711102962494\n",
      "Epoch: 3230, Training Loss: 0.15102650225162506, Test Loss: 0.14498481154441833\n",
      "Epoch: 3240, Training Loss: 0.15085500478744507, Test Loss: 0.14479348063468933\n",
      "Epoch: 3250, Training Loss: 0.15068426728248596, Test Loss: 0.14460307359695435\n",
      "Epoch: 3260, Training Loss: 0.1505143642425537, Test Loss: 0.14441365003585815\n",
      "Epoch: 3270, Training Loss: 0.15034523606300354, Test Loss: 0.1442251354455948\n",
      "Epoch: 3280, Training Loss: 0.15017688274383545, Test Loss: 0.14403757452964783\n",
      "Epoch: 3290, Training Loss: 0.15000930428504944, Test Loss: 0.1438509225845337\n",
      "Epoch: 3300, Training Loss: 0.1498425155878067, Test Loss: 0.14366517961025238\n",
      "Epoch: 3310, Training Loss: 0.14967648684978485, Test Loss: 0.1434803307056427\n",
      "Epoch: 3320, Training Loss: 0.1495112180709839, Test Loss: 0.14329639077186584\n",
      "Epoch: 3330, Training Loss: 0.1493467092514038, Test Loss: 0.14311335980892181\n",
      "Epoch: 3340, Training Loss: 0.14918294548988342, Test Loss: 0.14293120801448822\n",
      "Epoch: 3350, Training Loss: 0.14901992678642273, Test Loss: 0.14274992048740387\n",
      "Epoch: 3360, Training Loss: 0.14885765314102173, Test Loss: 0.14256951212882996\n",
      "Epoch: 3370, Training Loss: 0.14869607985019684, Test Loss: 0.14238996803760529\n",
      "Epoch: 3380, Training Loss: 0.14853526651859283, Test Loss: 0.14221127331256866\n",
      "Epoch: 3390, Training Loss: 0.14837516844272614, Test Loss: 0.14203347265720367\n",
      "Epoch: 3400, Training Loss: 0.14821578562259674, Test Loss: 0.14185647666454315\n",
      "Epoch: 3410, Training Loss: 0.14805710315704346, Test Loss: 0.14168034493923187\n",
      "Epoch: 3420, Training Loss: 0.14789912104606628, Test Loss: 0.14150504767894745\n",
      "Epoch: 3430, Training Loss: 0.1477418690919876, Test Loss: 0.1413305550813675\n",
      "Epoch: 3440, Training Loss: 0.14758530259132385, Test Loss: 0.14115691184997559\n",
      "Epoch: 3450, Training Loss: 0.14742940664291382, Test Loss: 0.14098407328128815\n",
      "Epoch: 3460, Training Loss: 0.1472741961479187, Test Loss: 0.14081203937530518\n",
      "Epoch: 3470, Training Loss: 0.1471197009086609, Test Loss: 0.14064082503318787\n",
      "Epoch: 3480, Training Loss: 0.1469658464193344, Test Loss: 0.14047038555145264\n",
      "Epoch: 3490, Training Loss: 0.14681267738342285, Test Loss: 0.14030076563358307\n",
      "Epoch: 3500, Training Loss: 0.14666016399860382, Test Loss: 0.14013192057609558\n",
      "Epoch: 3510, Training Loss: 0.1465083211660385, Test Loss: 0.13996383547782898\n",
      "Epoch: 3520, Training Loss: 0.14635714888572693, Test Loss: 0.13979655504226685\n",
      "Epoch: 3530, Training Loss: 0.14620660245418549, Test Loss: 0.1396300196647644\n",
      "Epoch: 3540, Training Loss: 0.14605669677257538, Test Loss: 0.13946427404880524\n",
      "Epoch: 3550, Training Loss: 0.1459074318408966, Test Loss: 0.13929927349090576\n",
      "Epoch: 3560, Training Loss: 0.14575882256031036, Test Loss: 0.13913501799106598\n",
      "Epoch: 3570, Training Loss: 0.14561083912849426, Test Loss: 0.1389715075492859\n",
      "Epoch: 3580, Training Loss: 0.1454634815454483, Test Loss: 0.13880877196788788\n",
      "Epoch: 3590, Training Loss: 0.14531674981117249, Test Loss: 0.13864672183990479\n",
      "Epoch: 3600, Training Loss: 0.1451706439256668, Test Loss: 0.13848546147346497\n",
      "Epoch: 3610, Training Loss: 0.1450251340866089, Test Loss: 0.13832490146160126\n",
      "Epoch: 3620, Training Loss: 0.1448802500963211, Test Loss: 0.13816504180431366\n",
      "Epoch: 3630, Training Loss: 0.14473596215248108, Test Loss: 0.13800592720508575\n",
      "Epoch: 3640, Training Loss: 0.1445923000574112, Test Loss: 0.13784751296043396\n",
      "Epoch: 3650, Training Loss: 0.14444921910762787, Test Loss: 0.13768979907035828\n",
      "Epoch: 3660, Training Loss: 0.1443067342042923, Test Loss: 0.1375328004360199\n",
      "Epoch: 3670, Training Loss: 0.1441648304462433, Test Loss: 0.13737650215625763\n",
      "Epoch: 3680, Training Loss: 0.14402350783348083, Test Loss: 0.13722088932991028\n",
      "Epoch: 3690, Training Loss: 0.14388281106948853, Test Loss: 0.13706597685813904\n",
      "Epoch: 3700, Training Loss: 0.143742635846138, Test Loss: 0.13691170513629913\n",
      "Epoch: 3710, Training Loss: 0.14360305666923523, Test Loss: 0.13675814867019653\n",
      "Epoch: 3720, Training Loss: 0.14346405863761902, Test Loss: 0.13660523295402527\n",
      "Epoch: 3730, Training Loss: 0.14332562685012817, Test Loss: 0.13645300269126892\n",
      "Epoch: 3740, Training Loss: 0.1431877464056015, Test Loss: 0.13630147278308868\n",
      "Epoch: 3750, Training Loss: 0.143050417304039, Test Loss: 0.1361505538225174\n",
      "Epoch: 3760, Training Loss: 0.14291366934776306, Test Loss: 0.13600030541419983\n",
      "Epoch: 3770, Training Loss: 0.1427774429321289, Test Loss: 0.1358507126569748\n",
      "Epoch: 3780, Training Loss: 0.14264178276062012, Test Loss: 0.1357017606496811\n",
      "Epoch: 3790, Training Loss: 0.1425066590309143, Test Loss: 0.13555346429347992\n",
      "Epoch: 3800, Training Loss: 0.14237207174301147, Test Loss: 0.1354057639837265\n",
      "Epoch: 3810, Training Loss: 0.14223803579807281, Test Loss: 0.1352587342262268\n",
      "Epoch: 3820, Training Loss: 0.14210452139377594, Test Loss: 0.13511233031749725\n",
      "Epoch: 3830, Training Loss: 0.14197154343128204, Test Loss: 0.13496655225753784\n",
      "Epoch: 3840, Training Loss: 0.14183908700942993, Test Loss: 0.13482138514518738\n",
      "Epoch: 3850, Training Loss: 0.1417071372270584, Test Loss: 0.13467682898044586\n",
      "Epoch: 3860, Training Loss: 0.14157573878765106, Test Loss: 0.1345328986644745\n",
      "Epoch: 3870, Training Loss: 0.1414448320865631, Test Loss: 0.13438956439495087\n",
      "Epoch: 3880, Training Loss: 0.14131444692611694, Test Loss: 0.1342468559741974\n",
      "Epoch: 3890, Training Loss: 0.14118456840515137, Test Loss: 0.13410471379756927\n",
      "Epoch: 3900, Training Loss: 0.14105519652366638, Test Loss: 0.1339631974697113\n",
      "Epoch: 3910, Training Loss: 0.140926331281662, Test Loss: 0.1338222473859787\n",
      "Epoch: 3920, Training Loss: 0.140797957777977, Test Loss: 0.13368192315101624\n",
      "Epoch: 3930, Training Loss: 0.1406700760126114, Test Loss: 0.13354215025901794\n",
      "Epoch: 3940, Training Loss: 0.14054268598556519, Test Loss: 0.1334029734134674\n",
      "Epoch: 3950, Training Loss: 0.14041578769683838, Test Loss: 0.13326437771320343\n",
      "Epoch: 3960, Training Loss: 0.14028939604759216, Test Loss: 0.13312633335590363\n",
      "Epoch: 3970, Training Loss: 0.14016346633434296, Test Loss: 0.13298887014389038\n",
      "Epoch: 3980, Training Loss: 0.14003801345825195, Test Loss: 0.1328519731760025\n",
      "Epoch: 3990, Training Loss: 0.13991305232048035, Test Loss: 0.13271564245224\n",
      "Epoch: 4000, Training Loss: 0.13978856801986694, Test Loss: 0.13257989287376404\n",
      "Epoch: 4010, Training Loss: 0.13966456055641174, Test Loss: 0.13244463503360748\n",
      "Epoch: 4020, Training Loss: 0.13954100012779236, Test Loss: 0.13230998814105988\n",
      "Epoch: 4030, Training Loss: 0.13941791653633118, Test Loss: 0.13217587769031525\n",
      "Epoch: 4040, Training Loss: 0.139295294880867, Test Loss: 0.1320423185825348\n",
      "Epoch: 4050, Training Loss: 0.13917312026023865, Test Loss: 0.13190926611423492\n",
      "Epoch: 4060, Training Loss: 0.1390514224767685, Test Loss: 0.13177677989006042\n",
      "Epoch: 4070, Training Loss: 0.13893017172813416, Test Loss: 0.13164480030536652\n",
      "Epoch: 4080, Training Loss: 0.13880935311317444, Test Loss: 0.13151340186595917\n",
      "Epoch: 4090, Training Loss: 0.13868899643421173, Test Loss: 0.13138246536254883\n",
      "Epoch: 4100, Training Loss: 0.13856910169124603, Test Loss: 0.13125211000442505\n",
      "Epoch: 4110, Training Loss: 0.13844963908195496, Test Loss: 0.13112224638462067\n",
      "Epoch: 4120, Training Loss: 0.1383306086063385, Test Loss: 0.13099290430545807\n",
      "Epoch: 4130, Training Loss: 0.13821201026439667, Test Loss: 0.13086409866809845\n",
      "Epoch: 4140, Training Loss: 0.13809385895729065, Test Loss: 0.13073576986789703\n",
      "Epoch: 4150, Training Loss: 0.13797612488269806, Test Loss: 0.1306079775094986\n",
      "Epoch: 4160, Training Loss: 0.13785883784294128, Test Loss: 0.13048066198825836\n",
      "Epoch: 4170, Training Loss: 0.13774198293685913, Test Loss: 0.1303538680076599\n",
      "Epoch: 4180, Training Loss: 0.1376255452632904, Test Loss: 0.13022758066654205\n",
      "Epoch: 4190, Training Loss: 0.1375095248222351, Test Loss: 0.1301017850637436\n",
      "Epoch: 4200, Training Loss: 0.13739390671253204, Test Loss: 0.12997646629810333\n",
      "Epoch: 4210, Training Loss: 0.137278750538826, Test Loss: 0.12985165417194366\n",
      "Epoch: 4220, Training Loss: 0.13716396689414978, Test Loss: 0.1297273337841034\n",
      "Epoch: 4230, Training Loss: 0.1370495855808258, Test Loss: 0.12960347533226013\n",
      "Epoch: 4240, Training Loss: 0.13693565130233765, Test Loss: 0.12948010861873627\n",
      "Epoch: 4250, Training Loss: 0.13682210445404053, Test Loss: 0.1293572187423706\n",
      "Epoch: 4260, Training Loss: 0.13670897483825684, Test Loss: 0.12923480570316315\n",
      "Epoch: 4270, Training Loss: 0.13659623265266418, Test Loss: 0.1291128695011139\n",
      "Epoch: 4280, Training Loss: 0.13648389279842377, Test Loss: 0.12899138033390045\n",
      "Epoch: 4290, Training Loss: 0.13637195527553558, Test Loss: 0.1288703978061676\n",
      "Epoch: 4300, Training Loss: 0.13626042008399963, Test Loss: 0.12874984741210938\n",
      "Epoch: 4310, Training Loss: 0.13614925742149353, Test Loss: 0.12862978875637054\n",
      "Epoch: 4320, Training Loss: 0.13603849709033966, Test Loss: 0.12851016223430634\n",
      "Epoch: 4330, Training Loss: 0.13592812418937683, Test Loss: 0.12839098274707794\n",
      "Epoch: 4340, Training Loss: 0.13581813871860504, Test Loss: 0.12827228009700775\n",
      "Epoch: 4350, Training Loss: 0.1357085257768631, Test Loss: 0.12815400958061218\n",
      "Epoch: 4360, Training Loss: 0.1355993151664734, Test Loss: 0.12803620100021362\n",
      "Epoch: 4370, Training Loss: 0.13549046218395233, Test Loss: 0.1279188096523285\n",
      "Epoch: 4380, Training Loss: 0.13538199663162231, Test Loss: 0.12780191004276276\n",
      "Epoch: 4390, Training Loss: 0.13527390360832214, Test Loss: 0.12768539786338806\n",
      "Epoch: 4400, Training Loss: 0.13516616821289062, Test Loss: 0.12756936252117157\n",
      "Epoch: 4410, Training Loss: 0.13505882024765015, Test Loss: 0.1274537444114685\n",
      "Epoch: 4420, Training Loss: 0.13495184481143951, Test Loss: 0.12733855843544006\n",
      "Epoch: 4430, Training Loss: 0.13484522700309753, Test Loss: 0.12722380459308624\n",
      "Epoch: 4440, Training Loss: 0.1347389668226242, Test Loss: 0.12710948288440704\n",
      "Epoch: 4450, Training Loss: 0.13463307917118073, Test Loss: 0.12699557840824127\n",
      "Epoch: 4460, Training Loss: 0.1345275491476059, Test Loss: 0.12688207626342773\n",
      "Epoch: 4470, Training Loss: 0.13442237675189972, Test Loss: 0.12676902115345\n",
      "Epoch: 4480, Training Loss: 0.1343175619840622, Test Loss: 0.12665638327598572\n",
      "Epoch: 4490, Training Loss: 0.13421308994293213, Test Loss: 0.12654414772987366\n",
      "Epoch: 4500, Training Loss: 0.1341089904308319, Test Loss: 0.1264323592185974\n",
      "Epoch: 4510, Training Loss: 0.13400521874427795, Test Loss: 0.12632092833518982\n",
      "Epoch: 4520, Training Loss: 0.13390180468559265, Test Loss: 0.12620992958545685\n",
      "Epoch: 4530, Training Loss: 0.133798748254776, Test Loss: 0.1260993331670761\n",
      "Epoch: 4540, Training Loss: 0.13369601964950562, Test Loss: 0.1259891390800476\n",
      "Epoch: 4550, Training Loss: 0.13359364867210388, Test Loss: 0.12587936222553253\n",
      "Epoch: 4560, Training Loss: 0.1334916055202484, Test Loss: 0.1257699728012085\n",
      "Epoch: 4570, Training Loss: 0.1333898901939392, Test Loss: 0.1256609857082367\n",
      "Epoch: 4580, Training Loss: 0.13328853249549866, Test Loss: 0.12555237114429474\n",
      "Epoch: 4590, Training Loss: 0.13318748772144318, Test Loss: 0.1254441738128662\n",
      "Epoch: 4600, Training Loss: 0.13308680057525635, Test Loss: 0.12533634901046753\n",
      "Epoch: 4610, Training Loss: 0.13298644125461578, Test Loss: 0.1252289116382599\n",
      "Epoch: 4620, Training Loss: 0.1328863948583603, Test Loss: 0.12512187659740448\n",
      "Epoch: 4630, Training Loss: 0.13278667628765106, Test Loss: 0.12501521408557892\n",
      "Epoch: 4640, Training Loss: 0.1326873004436493, Test Loss: 0.1249089315533638\n",
      "Epoch: 4650, Training Loss: 0.1325882375240326, Test Loss: 0.12480302155017853\n",
      "Epoch: 4660, Training Loss: 0.13248948752880096, Test Loss: 0.1246974915266037\n",
      "Epoch: 4670, Training Loss: 0.1323910802602768, Test Loss: 0.12459234893321991\n",
      "Epoch: 4680, Training Loss: 0.1322929859161377, Test Loss: 0.12448757886886597\n",
      "Epoch: 4690, Training Loss: 0.13219518959522247, Test Loss: 0.12438315898180008\n",
      "Epoch: 4700, Training Loss: 0.13209772109985352, Test Loss: 0.12427913397550583\n",
      "Epoch: 4710, Training Loss: 0.13200058043003082, Test Loss: 0.12417545169591904\n",
      "Epoch: 4720, Training Loss: 0.1319037228822708, Test Loss: 0.12407214194536209\n",
      "Epoch: 4730, Training Loss: 0.13180719316005707, Test Loss: 0.1239691898226738\n",
      "Epoch: 4740, Training Loss: 0.1317109763622284, Test Loss: 0.12386661767959595\n",
      "Epoch: 4750, Training Loss: 0.1316150426864624, Test Loss: 0.12376438826322556\n",
      "Epoch: 4760, Training Loss: 0.13151943683624268, Test Loss: 0.12366252392530441\n",
      "Epoch: 4770, Training Loss: 0.13142412900924683, Test Loss: 0.12356100231409073\n",
      "Epoch: 4780, Training Loss: 0.13132913410663605, Test Loss: 0.1234598308801651\n",
      "Epoch: 4790, Training Loss: 0.13123442232608795, Test Loss: 0.12335903942584991\n",
      "Epoch: 4800, Training Loss: 0.13114002346992493, Test Loss: 0.1232585683465004\n",
      "Epoch: 4810, Training Loss: 0.13104590773582458, Test Loss: 0.12315845489501953\n",
      "Epoch: 4820, Training Loss: 0.13095209002494812, Test Loss: 0.12305867671966553\n",
      "Epoch: 4830, Training Loss: 0.13085855543613434, Test Loss: 0.12295926362276077\n",
      "Epoch: 4840, Training Loss: 0.13076534867286682, Test Loss: 0.12286017835140228\n",
      "Epoch: 4850, Training Loss: 0.1306724101305008, Test Loss: 0.12276143580675125\n",
      "Epoch: 4860, Training Loss: 0.13057975471019745, Test Loss: 0.12266302853822708\n",
      "Epoch: 4870, Training Loss: 0.13048739731311798, Test Loss: 0.12256494909524918\n",
      "Epoch: 4880, Training Loss: 0.1303953230381012, Test Loss: 0.12246721982955933\n",
      "Epoch: 4890, Training Loss: 0.1303035467863083, Test Loss: 0.12236981838941574\n",
      "Epoch: 4900, Training Loss: 0.13021203875541687, Test Loss: 0.12227275967597961\n",
      "Epoch: 4910, Training Loss: 0.13012082874774933, Test Loss: 0.12217599153518677\n",
      "Epoch: 4920, Training Loss: 0.1300298571586609, Test Loss: 0.12207958102226257\n",
      "Epoch: 4930, Training Loss: 0.1299392133951187, Test Loss: 0.12198349833488464\n",
      "Epoch: 4940, Training Loss: 0.12984883785247803, Test Loss: 0.12188771367073059\n",
      "Epoch: 4950, Training Loss: 0.12975873053073883, Test Loss: 0.12179229408502579\n",
      "Epoch: 4960, Training Loss: 0.12966889142990112, Test Loss: 0.12169715762138367\n",
      "Epoch: 4970, Training Loss: 0.1295793354511261, Test Loss: 0.12160234898328781\n",
      "Epoch: 4980, Training Loss: 0.12949004769325256, Test Loss: 0.12150786072015762\n",
      "Epoch: 4990, Training Loss: 0.12940102815628052, Test Loss: 0.1214136928319931\n",
      "Epoch: 5000, Training Loss: 0.12931227684020996, Test Loss: 0.12131982296705246\n",
      "Epoch: 5010, Training Loss: 0.1292238086462021, Test Loss: 0.12122628092765808\n",
      "Epoch: 5020, Training Loss: 0.1291355937719345, Test Loss: 0.12113305181264877\n",
      "Epoch: 5030, Training Loss: 0.12904763221740723, Test Loss: 0.12104012072086334\n",
      "Epoch: 5040, Training Loss: 0.12895996868610382, Test Loss: 0.12094751000404358\n",
      "Epoch: 5050, Training Loss: 0.1288725584745407, Test Loss: 0.12085520476102829\n",
      "Epoch: 5060, Training Loss: 0.1287853866815567, Test Loss: 0.12076318264007568\n",
      "Epoch: 5070, Training Loss: 0.12869849801063538, Test Loss: 0.12067148089408875\n",
      "Epoch: 5080, Training Loss: 0.12861184775829315, Test Loss: 0.12058006972074509\n",
      "Epoch: 5090, Training Loss: 0.12852546572685242, Test Loss: 0.1204889789223671\n",
      "Epoch: 5100, Training Loss: 0.12843935191631317, Test Loss: 0.12039816379547119\n",
      "Epoch: 5110, Training Loss: 0.12835346162319183, Test Loss: 0.12030764669179916\n",
      "Epoch: 5120, Training Loss: 0.12826786935329437, Test Loss: 0.12021742016077042\n",
      "Epoch: 5130, Training Loss: 0.12818250060081482, Test Loss: 0.12012751400470734\n",
      "Epoch: 5140, Training Loss: 0.12809737026691437, Test Loss: 0.12003788352012634\n",
      "Epoch: 5150, Training Loss: 0.1280125230550766, Test Loss: 0.11994853615760803\n",
      "Epoch: 5160, Training Loss: 0.12792789936065674, Test Loss: 0.1198594942688942\n",
      "Epoch: 5170, Training Loss: 0.12784352898597717, Test Loss: 0.11977073550224304\n",
      "Epoch: 5180, Training Loss: 0.1277594119310379, Test Loss: 0.11968225985765457\n",
      "Epoch: 5190, Training Loss: 0.12767554819583893, Test Loss: 0.11959405243396759\n",
      "Epoch: 5200, Training Loss: 0.12759192287921906, Test Loss: 0.11950615793466568\n",
      "Epoch: 5210, Training Loss: 0.1275085210800171, Test Loss: 0.11941853910684586\n",
      "Epoch: 5220, Training Loss: 0.1274253875017166, Test Loss: 0.11933119595050812\n",
      "Epoch: 5230, Training Loss: 0.12734247744083405, Test Loss: 0.11924411356449127\n",
      "Epoch: 5240, Training Loss: 0.12725980579853058, Test Loss: 0.1191573292016983\n",
      "Epoch: 5250, Training Loss: 0.1271773874759674, Test Loss: 0.11907081305980682\n",
      "Epoch: 5260, Training Loss: 0.12709519267082214, Test Loss: 0.11898460239171982\n",
      "Epoch: 5270, Training Loss: 0.12701325118541718, Test Loss: 0.11889862269163132\n",
      "Epoch: 5280, Training Loss: 0.12693151831626892, Test Loss: 0.1188129261136055\n",
      "Epoch: 5290, Training Loss: 0.12685003876686096, Test Loss: 0.11872752010822296\n",
      "Epoch: 5300, Training Loss: 0.1267687827348709, Test Loss: 0.11864237487316132\n",
      "Epoch: 5310, Training Loss: 0.12668775022029877, Test Loss: 0.11855749785900116\n",
      "Epoch: 5320, Training Loss: 0.12660697102546692, Test Loss: 0.11847288906574249\n",
      "Epoch: 5330, Training Loss: 0.1265263855457306, Test Loss: 0.11838851124048233\n",
      "Epoch: 5340, Training Loss: 0.12644606828689575, Test Loss: 0.11830446124076843\n",
      "Epoch: 5350, Training Loss: 0.12636595964431763, Test Loss: 0.11822062730789185\n",
      "Epoch: 5360, Training Loss: 0.1262860745191574, Test Loss: 0.11813706904649734\n",
      "Epoch: 5370, Training Loss: 0.1262064129114151, Test Loss: 0.11805377155542374\n",
      "Epoch: 5380, Training Loss: 0.1261269897222519, Test Loss: 0.11797071993350983\n",
      "Epoch: 5390, Training Loss: 0.1260477751493454, Test Loss: 0.1178879514336586\n",
      "Epoch: 5400, Training Loss: 0.1259687840938568, Test Loss: 0.11780542880296707\n",
      "Epoch: 5410, Training Loss: 0.12589003145694733, Test Loss: 0.11772313714027405\n",
      "Epoch: 5420, Training Loss: 0.12581147253513336, Test Loss: 0.1176411360502243\n",
      "Epoch: 5430, Training Loss: 0.1257331520318985, Test Loss: 0.11755938082933426\n",
      "Epoch: 5440, Training Loss: 0.12565502524375916, Test Loss: 0.11747787892818451\n",
      "Epoch: 5450, Training Loss: 0.1255771368741989, Test Loss: 0.11739661544561386\n",
      "Epoch: 5460, Training Loss: 0.12549945712089539, Test Loss: 0.11731560528278351\n",
      "Epoch: 5470, Training Loss: 0.12542198598384857, Test Loss: 0.11723484098911285\n",
      "Epoch: 5480, Training Loss: 0.12534473836421967, Test Loss: 0.11715433746576309\n",
      "Epoch: 5490, Training Loss: 0.12526771426200867, Test Loss: 0.11707407236099243\n",
      "Epoch: 5500, Training Loss: 0.1251908838748932, Test Loss: 0.11699405312538147\n",
      "Epoch: 5510, Training Loss: 0.12511426210403442, Test Loss: 0.11691427230834961\n",
      "Epoch: 5520, Training Loss: 0.12503786385059357, Test Loss: 0.11683473736047745\n",
      "Epoch: 5530, Training Loss: 0.12496167421340942, Test Loss: 0.11675545573234558\n",
      "Epoch: 5540, Training Loss: 0.1248856708407402, Test Loss: 0.11667639762163162\n",
      "Epoch: 5550, Training Loss: 0.1248098835349083, Test Loss: 0.11659758538007736\n",
      "Epoch: 5560, Training Loss: 0.1247343122959137, Test Loss: 0.11651899665594101\n",
      "Epoch: 5570, Training Loss: 0.12465893477201462, Test Loss: 0.11644066125154495\n",
      "Epoch: 5580, Training Loss: 0.12458378821611404, Test Loss: 0.1163625568151474\n",
      "Epoch: 5590, Training Loss: 0.1245088130235672, Test Loss: 0.11628470569849014\n",
      "Epoch: 5600, Training Loss: 0.12443404644727707, Test Loss: 0.1162070631980896\n",
      "Epoch: 5610, Training Loss: 0.12435949593782425, Test Loss: 0.11612966656684875\n",
      "Epoch: 5620, Training Loss: 0.12428512424230576, Test Loss: 0.11605249345302582\n",
      "Epoch: 5630, Training Loss: 0.12421096861362457, Test Loss: 0.11597555875778198\n",
      "Epoch: 5640, Training Loss: 0.12413700670003891, Test Loss: 0.11589883267879486\n",
      "Epoch: 5650, Training Loss: 0.12406325340270996, Test Loss: 0.11582236737012863\n",
      "Epoch: 5660, Training Loss: 0.12398968636989594, Test Loss: 0.11574610322713852\n",
      "Epoch: 5670, Training Loss: 0.12391630560159683, Test Loss: 0.11567007750272751\n",
      "Epoch: 5680, Training Loss: 0.12384311854839325, Test Loss: 0.1155942752957344\n",
      "Epoch: 5690, Training Loss: 0.12377014756202698, Test Loss: 0.11551870405673981\n",
      "Epoch: 5700, Training Loss: 0.12369735538959503, Test Loss: 0.11544332653284073\n",
      "Epoch: 5710, Training Loss: 0.1236247643828392, Test Loss: 0.11536820977926254\n",
      "Epoch: 5720, Training Loss: 0.1235523670911789, Test Loss: 0.11529329419136047\n",
      "Epoch: 5730, Training Loss: 0.12348014861345291, Test Loss: 0.11521859467029572\n",
      "Epoch: 5740, Training Loss: 0.12340812385082245, Test Loss: 0.11514411121606827\n",
      "Epoch: 5750, Training Loss: 0.12333627790212631, Test Loss: 0.11506985127925873\n",
      "Epoch: 5760, Training Loss: 0.12326465547084808, Test Loss: 0.11499582231044769\n",
      "Epoch: 5770, Training Loss: 0.12319318950176239, Test Loss: 0.11492198705673218\n",
      "Epoch: 5780, Training Loss: 0.12312192469835281, Test Loss: 0.11484838277101517\n",
      "Epoch: 5790, Training Loss: 0.12305083125829697, Test Loss: 0.11477497965097427\n",
      "Epoch: 5800, Training Loss: 0.12297993153333664, Test Loss: 0.11470180004835129\n",
      "Epoch: 5810, Training Loss: 0.12290921807289124, Test Loss: 0.11462882161140442\n",
      "Epoch: 5820, Training Loss: 0.12283868342638016, Test Loss: 0.11455608904361725\n",
      "Epoch: 5830, Training Loss: 0.1227683275938034, Test Loss: 0.11448352038860321\n",
      "Epoch: 5840, Training Loss: 0.12269817292690277, Test Loss: 0.11441117525100708\n",
      "Epoch: 5850, Training Loss: 0.12262818217277527, Test Loss: 0.11433906853199005\n",
      "Epoch: 5860, Training Loss: 0.12255838513374329, Test Loss: 0.11426713317632675\n",
      "Epoch: 5870, Training Loss: 0.12248875200748444, Test Loss: 0.11419540643692017\n",
      "Epoch: 5880, Training Loss: 0.12241929769515991, Test Loss: 0.11412390321493149\n",
      "Epoch: 5890, Training Loss: 0.12235003709793091, Test Loss: 0.11405258625745773\n",
      "Epoch: 5900, Training Loss: 0.12228093296289444, Test Loss: 0.11398149281740189\n",
      "Epoch: 5910, Training Loss: 0.12221202254295349, Test Loss: 0.11391057819128036\n",
      "Epoch: 5920, Training Loss: 0.12214327603578568, Test Loss: 0.11383988708257675\n",
      "Epoch: 5930, Training Loss: 0.12207471579313278, Test Loss: 0.11376939713954926\n",
      "Epoch: 5940, Training Loss: 0.12200633436441422, Test Loss: 0.11369910836219788\n",
      "Epoch: 5950, Training Loss: 0.12193810194730759, Test Loss: 0.11362900584936142\n",
      "Epoch: 5960, Training Loss: 0.12187007069587708, Test Loss: 0.11355910450220108\n",
      "Epoch: 5970, Training Loss: 0.1218022033572197, Test Loss: 0.11348941177129745\n",
      "Epoch: 5980, Training Loss: 0.12173449993133545, Test Loss: 0.11341991275548935\n",
      "Epoch: 5990, Training Loss: 0.12166697531938553, Test Loss: 0.11335059255361557\n",
      "Epoch: 6000, Training Loss: 0.12159959971904755, Test Loss: 0.1132814809679985\n",
      "Epoch: 6010, Training Loss: 0.12153241038322449, Test Loss: 0.11321255564689636\n",
      "Epoch: 6020, Training Loss: 0.12146538496017456, Test Loss: 0.11314383149147034\n",
      "Epoch: 6030, Training Loss: 0.12139853835105896, Test Loss: 0.11307530105113983\n",
      "Epoch: 6040, Training Loss: 0.12133185565471649, Test Loss: 0.11300697177648544\n",
      "Epoch: 6050, Training Loss: 0.12126532196998596, Test Loss: 0.11293881386518478\n",
      "Epoch: 6060, Training Loss: 0.12119898200035095, Test Loss: 0.11287084221839905\n",
      "Epoch: 6070, Training Loss: 0.12113277614116669, Test Loss: 0.11280307173728943\n",
      "Epoch: 6080, Training Loss: 0.12106674909591675, Test Loss: 0.11273550242185593\n",
      "Epoch: 6090, Training Loss: 0.12100088596343994, Test Loss: 0.11266809701919556\n",
      "Epoch: 6100, Training Loss: 0.12093518674373627, Test Loss: 0.11260087043046951\n",
      "Epoch: 6110, Training Loss: 0.12086965888738632, Test Loss: 0.11253386735916138\n",
      "Epoch: 6120, Training Loss: 0.12080427259206772, Test Loss: 0.11246702820062637\n",
      "Epoch: 6130, Training Loss: 0.12073906511068344, Test Loss: 0.11240038275718689\n",
      "Epoch: 6140, Training Loss: 0.1206740066409111, Test Loss: 0.11233390122652054\n",
      "Epoch: 6150, Training Loss: 0.1206091120839119, Test Loss: 0.11226761341094971\n",
      "Epoch: 6160, Training Loss: 0.12054437398910522, Test Loss: 0.11220152676105499\n",
      "Epoch: 6170, Training Loss: 0.12047979980707169, Test Loss: 0.11213558912277222\n",
      "Epoch: 6180, Training Loss: 0.12041538208723068, Test Loss: 0.11206985265016556\n",
      "Epoch: 6190, Training Loss: 0.12035112828016281, Test Loss: 0.11200427263975143\n",
      "Epoch: 6200, Training Loss: 0.12028700858354568, Test Loss: 0.11193888634443283\n",
      "Epoch: 6210, Training Loss: 0.12022306025028229, Test Loss: 0.11187367886304855\n",
      "Epoch: 6220, Training Loss: 0.12015926092863083, Test Loss: 0.1118086576461792\n",
      "Epoch: 6230, Training Loss: 0.1200956255197525, Test Loss: 0.11174379289150238\n",
      "Epoch: 6240, Training Loss: 0.12003213912248611, Test Loss: 0.11167910695075989\n",
      "Epoch: 6250, Training Loss: 0.11996880173683167, Test Loss: 0.11161462217569351\n",
      "Epoch: 6260, Training Loss: 0.11990562081336975, Test Loss: 0.11155030131340027\n",
      "Epoch: 6270, Training Loss: 0.11984258890151978, Test Loss: 0.11148614436388016\n",
      "Epoch: 6280, Training Loss: 0.11977971345186234, Test Loss: 0.11142215132713318\n",
      "Epoch: 6290, Training Loss: 0.11971698701381683, Test Loss: 0.11135835200548172\n",
      "Epoch: 6300, Training Loss: 0.11965441703796387, Test Loss: 0.1112946942448616\n",
      "Epoch: 6310, Training Loss: 0.11959198862314224, Test Loss: 0.1112312451004982\n",
      "Epoch: 6320, Training Loss: 0.11952970921993256, Test Loss: 0.11116795986890793\n",
      "Epoch: 6330, Training Loss: 0.11946757137775421, Test Loss: 0.11110483109951019\n",
      "Epoch: 6340, Training Loss: 0.1194055899977684, Test Loss: 0.11104187369346619\n",
      "Epoch: 6350, Training Loss: 0.11934375762939453, Test Loss: 0.11097907274961472\n",
      "Epoch: 6360, Training Loss: 0.1192820817232132, Test Loss: 0.11091645807027817\n",
      "Epoch: 6370, Training Loss: 0.11922052502632141, Test Loss: 0.11085399985313416\n",
      "Epoch: 6380, Training Loss: 0.11915913224220276, Test Loss: 0.11079170554876328\n",
      "Epoch: 6390, Training Loss: 0.11909787356853485, Test Loss: 0.11072958260774612\n",
      "Epoch: 6400, Training Loss: 0.11903677135705948, Test Loss: 0.1106676310300827\n",
      "Epoch: 6410, Training Loss: 0.11897581070661545, Test Loss: 0.11060583591461182\n",
      "Epoch: 6420, Training Loss: 0.11891499161720276, Test Loss: 0.11054419726133347\n",
      "Epoch: 6430, Training Loss: 0.11885429918766022, Test Loss: 0.11048273742198944\n",
      "Epoch: 6440, Training Loss: 0.11879377067089081, Test Loss: 0.11042141169309616\n",
      "Epoch: 6450, Training Loss: 0.11873338371515274, Test Loss: 0.1103602722287178\n",
      "Epoch: 6460, Training Loss: 0.11867313086986542, Test Loss: 0.11029928922653198\n",
      "Epoch: 6470, Training Loss: 0.11861302703619003, Test Loss: 0.1102384403347969\n",
      "Epoch: 6480, Training Loss: 0.1185530573129654, Test Loss: 0.11017780005931854\n",
      "Epoch: 6490, Training Loss: 0.1184932067990303, Test Loss: 0.11011729389429092\n",
      "Epoch: 6500, Training Loss: 0.11843352019786835, Test Loss: 0.11005692929029465\n",
      "Epoch: 6510, Training Loss: 0.11837396025657654, Test Loss: 0.1099967360496521\n",
      "Epoch: 6520, Training Loss: 0.11831453442573547, Test Loss: 0.10993669182062149\n",
      "Epoch: 6530, Training Loss: 0.11825526505708694, Test Loss: 0.10987681895494461\n",
      "Epoch: 6540, Training Loss: 0.11819611489772797, Test Loss: 0.10981711000204086\n",
      "Epoch: 6550, Training Loss: 0.11813710629940033, Test Loss: 0.10975752025842667\n",
      "Epoch: 6560, Training Loss: 0.11807823181152344, Test Loss: 0.1096981018781662\n",
      "Epoch: 6570, Training Loss: 0.11801949143409729, Test Loss: 0.10963884741067886\n",
      "Epoch: 6580, Training Loss: 0.11796089261770248, Test Loss: 0.10957974195480347\n",
      "Epoch: 6590, Training Loss: 0.11790242046117783, Test Loss: 0.10952078551054001\n",
      "Epoch: 6600, Training Loss: 0.11784408986568451, Test Loss: 0.1094619631767273\n",
      "Epoch: 6610, Training Loss: 0.11778588593006134, Test Loss: 0.1094033271074295\n",
      "Epoch: 6620, Training Loss: 0.11772781610488892, Test Loss: 0.10934481769800186\n",
      "Epoch: 6630, Training Loss: 0.11766988039016724, Test Loss: 0.10928646475076675\n",
      "Epoch: 6640, Training Loss: 0.11761206388473511, Test Loss: 0.10922825336456299\n",
      "Epoch: 6650, Training Loss: 0.11755439639091492, Test Loss: 0.10917020589113235\n",
      "Epoch: 6660, Training Loss: 0.11749684065580368, Test Loss: 0.10911230742931366\n",
      "Epoch: 6670, Training Loss: 0.11743943393230438, Test Loss: 0.10905454307794571\n",
      "Epoch: 6680, Training Loss: 0.11738214641809464, Test Loss: 0.1089969277381897\n",
      "Epoch: 6690, Training Loss: 0.11732498556375504, Test Loss: 0.10893945395946503\n",
      "Epoch: 6700, Training Loss: 0.11726796627044678, Test Loss: 0.10888215154409409\n",
      "Epoch: 6710, Training Loss: 0.11721107363700867, Test Loss: 0.1088249608874321\n",
      "Epoch: 6720, Training Loss: 0.11715429276227951, Test Loss: 0.10876791924238205\n",
      "Epoch: 6730, Training Loss: 0.1170976534485817, Test Loss: 0.10871104151010513\n",
      "Epoch: 6740, Training Loss: 0.11704112589359283, Test Loss: 0.10865432024002075\n",
      "Epoch: 6750, Training Loss: 0.11698473989963531, Test Loss: 0.10859771072864532\n",
      "Epoch: 6760, Training Loss: 0.11692845821380615, Test Loss: 0.10854125767946243\n",
      "Epoch: 6770, Training Loss: 0.11687233299016953, Test Loss: 0.10848495364189148\n",
      "Epoch: 6780, Training Loss: 0.11681631207466125, Test Loss: 0.10842876881361008\n",
      "Epoch: 6790, Training Loss: 0.11676042526960373, Test Loss: 0.10837271809577942\n",
      "Epoch: 6800, Training Loss: 0.11670465767383575, Test Loss: 0.10831684619188309\n",
      "Epoch: 6810, Training Loss: 0.11664902418851852, Test Loss: 0.1082611009478569\n",
      "Epoch: 6820, Training Loss: 0.11659348756074905, Test Loss: 0.10820548236370087\n",
      "Epoch: 6830, Training Loss: 0.11653809249401093, Test Loss: 0.10814999788999557\n",
      "Epoch: 6840, Training Loss: 0.11648280918598175, Test Loss: 0.10809466987848282\n",
      "Epoch: 6850, Training Loss: 0.11642765998840332, Test Loss: 0.10803946852684021\n",
      "Epoch: 6860, Training Loss: 0.11637263000011444, Test Loss: 0.10798440873622894\n",
      "Epoch: 6870, Training Loss: 0.11631770431995392, Test Loss: 0.10792947560548782\n",
      "Epoch: 6880, Training Loss: 0.11626291275024414, Test Loss: 0.10787470638751984\n",
      "Epoch: 6890, Training Loss: 0.11620824038982391, Test Loss: 0.10782003402709961\n",
      "Epoch: 6900, Training Loss: 0.11615367978811264, Test Loss: 0.10776551812887192\n",
      "Epoch: 6910, Training Loss: 0.11609926074743271, Test Loss: 0.10771115124225616\n",
      "Epoch: 6920, Training Loss: 0.11604493856430054, Test Loss: 0.10765688121318817\n",
      "Epoch: 6930, Training Loss: 0.11599072813987732, Test Loss: 0.10760277509689331\n",
      "Epoch: 6940, Training Loss: 0.11593665182590485, Test Loss: 0.1075487807393074\n",
      "Epoch: 6950, Training Loss: 0.11588269472122192, Test Loss: 0.10749494284391403\n",
      "Epoch: 6960, Training Loss: 0.11582884192466736, Test Loss: 0.10744120925664902\n",
      "Epoch: 6970, Training Loss: 0.11577511578798294, Test Loss: 0.10738762468099594\n",
      "Epoch: 6980, Training Loss: 0.11572150141000748, Test Loss: 0.10733415931463242\n",
      "Epoch: 6990, Training Loss: 0.11566799879074097, Test Loss: 0.10728085041046143\n",
      "Epoch: 7000, Training Loss: 0.11561460793018341, Test Loss: 0.10722766071557999\n",
      "Epoch: 7010, Training Loss: 0.1155613362789154, Test Loss: 0.1071745902299881\n",
      "Epoch: 7020, Training Loss: 0.11550819128751755, Test Loss: 0.10712162405252457\n",
      "Epoch: 7030, Training Loss: 0.11545515060424805, Test Loss: 0.10706881433725357\n",
      "Epoch: 7040, Training Loss: 0.1154022142291069, Test Loss: 0.10701614618301392\n",
      "Epoch: 7050, Training Loss: 0.1153494119644165, Test Loss: 0.10696358233690262\n",
      "Epoch: 7060, Training Loss: 0.11529669165611267, Test Loss: 0.10691116005182266\n",
      "Epoch: 7070, Training Loss: 0.11524412035942078, Test Loss: 0.10685887187719345\n",
      "Epoch: 7080, Training Loss: 0.11519163846969604, Test Loss: 0.1068066880106926\n",
      "Epoch: 7090, Training Loss: 0.11513925343751907, Test Loss: 0.10675463825464249\n",
      "Epoch: 7100, Training Loss: 0.11508700996637344, Test Loss: 0.10670272260904312\n",
      "Epoch: 7110, Training Loss: 0.11503487825393677, Test Loss: 0.10665091872215271\n",
      "Epoch: 7120, Training Loss: 0.11498283594846725, Test Loss: 0.10659924894571304\n",
      "Epoch: 7130, Training Loss: 0.1149308979511261, Test Loss: 0.10654769837856293\n",
      "Epoch: 7140, Training Loss: 0.11487909406423569, Test Loss: 0.10649626702070236\n",
      "Epoch: 7150, Training Loss: 0.11482739448547363, Test Loss: 0.10644496232271194\n",
      "Epoch: 7160, Training Loss: 0.11477579176425934, Test Loss: 0.10639376938343048\n",
      "Epoch: 7170, Training Loss: 0.11472431570291519, Test Loss: 0.10634271800518036\n",
      "Epoch: 7180, Training Loss: 0.11467292904853821, Test Loss: 0.10629177838563919\n",
      "Epoch: 7190, Training Loss: 0.11462164670228958, Test Loss: 0.10624095797538757\n",
      "Epoch: 7200, Training Loss: 0.1145704835653305, Test Loss: 0.1061902642250061\n",
      "Epoch: 7210, Training Loss: 0.11451943218708038, Test Loss: 0.10613968968391418\n",
      "Epoch: 7220, Training Loss: 0.11446847021579742, Test Loss: 0.10608923435211182\n",
      "Epoch: 7230, Training Loss: 0.11441762745380402, Test Loss: 0.1060388833284378\n",
      "Epoch: 7240, Training Loss: 0.11436689645051956, Test Loss: 0.10598866641521454\n",
      "Epoch: 7250, Training Loss: 0.11431626230478287, Test Loss: 0.10593856871128082\n",
      "Epoch: 7260, Training Loss: 0.11426572501659393, Test Loss: 0.10588858276605606\n",
      "Epoch: 7270, Training Loss: 0.11421529948711395, Test Loss: 0.10583872348070145\n",
      "Epoch: 7280, Training Loss: 0.11416497826576233, Test Loss: 0.10578898340463638\n",
      "Epoch: 7290, Training Loss: 0.11411476880311966, Test Loss: 0.10573934018611908\n",
      "Epoch: 7300, Training Loss: 0.11406465619802475, Test Loss: 0.10568983852863312\n",
      "Epoch: 7310, Training Loss: 0.114014632999897, Test Loss: 0.10564044117927551\n",
      "Epoch: 7320, Training Loss: 0.11396472156047821, Test Loss: 0.10559114813804626\n",
      "Epoch: 7330, Training Loss: 0.11391492933034897, Test Loss: 0.10554197430610657\n",
      "Epoch: 7340, Training Loss: 0.11386521905660629, Test Loss: 0.10549291968345642\n",
      "Epoch: 7350, Training Loss: 0.11381562054157257, Test Loss: 0.10544398427009583\n",
      "Epoch: 7360, Training Loss: 0.11376611888408661, Test Loss: 0.10539516061544418\n",
      "Epoch: 7370, Training Loss: 0.113716721534729, Test Loss: 0.1053464487195015\n",
      "Epoch: 7380, Training Loss: 0.11366742104291916, Test Loss: 0.10529784113168716\n",
      "Epoch: 7390, Training Loss: 0.11361823230981827, Test Loss: 0.10524937510490417\n",
      "Epoch: 7400, Training Loss: 0.11356913298368454, Test Loss: 0.10520099103450775\n",
      "Epoch: 7410, Training Loss: 0.11352013051509857, Test Loss: 0.10515273362398148\n",
      "Epoch: 7420, Training Loss: 0.11347123980522156, Test Loss: 0.10510457307100296\n",
      "Epoch: 7430, Training Loss: 0.1134224385023117, Test Loss: 0.10505654662847519\n",
      "Epoch: 7440, Training Loss: 0.11337372660636902, Test Loss: 0.10500860214233398\n",
      "Epoch: 7450, Training Loss: 0.11332512646913528, Test Loss: 0.10496079176664352\n",
      "Epoch: 7460, Training Loss: 0.11327662318944931, Test Loss: 0.10491307824850082\n",
      "Epoch: 7470, Training Loss: 0.1132282167673111, Test Loss: 0.10486548393964767\n",
      "Epoch: 7480, Training Loss: 0.11317991465330124, Test Loss: 0.10481797903776169\n",
      "Epoch: 7490, Training Loss: 0.11313170194625854, Test Loss: 0.10477060079574585\n",
      "Epoch: 7500, Training Loss: 0.11308358609676361, Test Loss: 0.10472333431243896\n",
      "Epoch: 7510, Training Loss: 0.11303555965423584, Test Loss: 0.10467614978551865\n",
      "Epoch: 7520, Training Loss: 0.11298765242099762, Test Loss: 0.10462910681962967\n",
      "Epoch: 7530, Training Loss: 0.11293981969356537, Test Loss: 0.10458215326070786\n",
      "Epoch: 7540, Training Loss: 0.11289208382368088, Test Loss: 0.1045353040099144\n",
      "Epoch: 7550, Training Loss: 0.11284445226192474, Test Loss: 0.1044885665178299\n",
      "Epoch: 7560, Training Loss: 0.11279691755771637, Test Loss: 0.10444193333387375\n",
      "Epoch: 7570, Training Loss: 0.11274946480989456, Test Loss: 0.10439540445804596\n",
      "Epoch: 7580, Training Loss: 0.11270210891962051, Test Loss: 0.10434897243976593\n",
      "Epoch: 7590, Training Loss: 0.11265485733747482, Test Loss: 0.10430265963077545\n",
      "Epoch: 7600, Training Loss: 0.1126076877117157, Test Loss: 0.10425644367933273\n",
      "Epoch: 7610, Training Loss: 0.11256062239408493, Test Loss: 0.10421034693717957\n",
      "Epoch: 7620, Training Loss: 0.11251363903284073, Test Loss: 0.10416434705257416\n",
      "Epoch: 7630, Training Loss: 0.11246675252914429, Test Loss: 0.10411844402551651\n",
      "Epoch: 7640, Training Loss: 0.11241995543241501, Test Loss: 0.10407265275716782\n",
      "Epoch: 7650, Training Loss: 0.11237326264381409, Test Loss: 0.10402694344520569\n",
      "Epoch: 7660, Training Loss: 0.11232664436101913, Test Loss: 0.10398136079311371\n",
      "Epoch: 7670, Training Loss: 0.11228013038635254, Test Loss: 0.1039358600974083\n",
      "Epoch: 7680, Training Loss: 0.1122337058186531, Test Loss: 0.10389048606157303\n",
      "Epoch: 7690, Training Loss: 0.11218737065792084, Test Loss: 0.10384517908096313\n",
      "Epoch: 7700, Training Loss: 0.11214112490415573, Test Loss: 0.10379999876022339\n",
      "Epoch: 7710, Training Loss: 0.11209496110677719, Test Loss: 0.1037549152970314\n",
      "Epoch: 7720, Training Loss: 0.11204889416694641, Test Loss: 0.10370992124080658\n",
      "Epoch: 7730, Training Loss: 0.11200292408466339, Test Loss: 0.10366503149271011\n",
      "Epoch: 7740, Training Loss: 0.11195702850818634, Test Loss: 0.1036202609539032\n",
      "Epoch: 7750, Training Loss: 0.11191123723983765, Test Loss: 0.10357557237148285\n",
      "Epoch: 7760, Training Loss: 0.11186553537845612, Test Loss: 0.10353099554777145\n",
      "Epoch: 7770, Training Loss: 0.11181990802288055, Test Loss: 0.10348649322986603\n",
      "Epoch: 7780, Training Loss: 0.11177437007427216, Test Loss: 0.10344209522008896\n",
      "Epoch: 7790, Training Loss: 0.11172892898321152, Test Loss: 0.10339781641960144\n",
      "Epoch: 7800, Training Loss: 0.11168357729911804, Test Loss: 0.10335361212491989\n",
      "Epoch: 7810, Training Loss: 0.11163830757141113, Test Loss: 0.1033094972372055\n",
      "Epoch: 7820, Training Loss: 0.11159311980009079, Test Loss: 0.10326550155878067\n",
      "Epoch: 7830, Training Loss: 0.1115480363368988, Test Loss: 0.1032216027379036\n",
      "Epoch: 7840, Training Loss: 0.11150301247835159, Test Loss: 0.10317779332399368\n",
      "Epoch: 7850, Training Loss: 0.11145808547735214, Test Loss: 0.10313407331705093\n",
      "Epoch: 7860, Training Loss: 0.11141325533390045, Test Loss: 0.10309046506881714\n",
      "Epoch: 7870, Training Loss: 0.11136849969625473, Test Loss: 0.10304693877696991\n",
      "Epoch: 7880, Training Loss: 0.11132384836673737, Test Loss: 0.10300350934267044\n",
      "Epoch: 7890, Training Loss: 0.11127925664186478, Test Loss: 0.10296017676591873\n",
      "Epoch: 7900, Training Loss: 0.11123475432395935, Test Loss: 0.10291692614555359\n",
      "Epoch: 7910, Training Loss: 0.11119035631418228, Test Loss: 0.1028738021850586\n",
      "Epoch: 7920, Training Loss: 0.11114602535963058, Test Loss: 0.10283074527978897\n",
      "Epoch: 7930, Training Loss: 0.11110177636146545, Test Loss: 0.10278777778148651\n",
      "Epoch: 7940, Training Loss: 0.11105762422084808, Test Loss: 0.102744922041893\n",
      "Epoch: 7950, Training Loss: 0.11101356148719788, Test Loss: 0.10270215570926666\n",
      "Epoch: 7960, Training Loss: 0.11096955835819244, Test Loss: 0.10265947878360748\n",
      "Epoch: 7970, Training Loss: 0.11092563718557358, Test Loss: 0.10261687636375427\n",
      "Epoch: 7980, Training Loss: 0.11088182777166367, Test Loss: 0.10257439315319061\n",
      "Epoch: 7990, Training Loss: 0.11083807051181793, Test Loss: 0.10253199189901352\n",
      "Epoch: 8000, Training Loss: 0.11079442501068115, Test Loss: 0.10248968005180359\n",
      "Epoch: 8010, Training Loss: 0.11075083166360855, Test Loss: 0.10244744271039963\n",
      "Epoch: 8020, Training Loss: 0.11070733517408371, Test Loss: 0.10240533202886581\n",
      "Epoch: 8030, Training Loss: 0.11066392809152603, Test Loss: 0.10236326605081558\n",
      "Epoch: 8040, Training Loss: 0.11062058806419373, Test Loss: 0.1023213267326355\n",
      "Epoch: 8050, Training Loss: 0.11057732254266739, Test Loss: 0.10227946937084198\n",
      "Epoch: 8060, Training Loss: 0.11053416132926941, Test Loss: 0.10223768651485443\n",
      "Epoch: 8070, Training Loss: 0.1104910671710968, Test Loss: 0.10219601541757584\n",
      "Epoch: 8080, Training Loss: 0.11044805496931076, Test Loss: 0.1021544337272644\n",
      "Epoch: 8090, Training Loss: 0.11040512472391129, Test Loss: 0.10211291164159775\n",
      "Epoch: 8100, Training Loss: 0.11036226153373718, Test Loss: 0.10207149386405945\n",
      "Epoch: 8110, Training Loss: 0.11031948775053024, Test Loss: 0.1020301803946495\n",
      "Epoch: 8120, Training Loss: 0.11027680337429047, Test Loss: 0.10198892652988434\n",
      "Epoch: 8130, Training Loss: 0.11023418605327606, Test Loss: 0.10194778442382812\n",
      "Epoch: 8140, Training Loss: 0.11019164323806763, Test Loss: 0.10190670192241669\n",
      "Epoch: 8150, Training Loss: 0.11014918982982635, Test Loss: 0.1018657237291336\n",
      "Epoch: 8160, Training Loss: 0.11010681092739105, Test Loss: 0.10182484239339828\n",
      "Epoch: 8170, Training Loss: 0.11006450653076172, Test Loss: 0.10178402066230774\n",
      "Epoch: 8180, Training Loss: 0.11002228409051895, Test Loss: 0.10174331068992615\n",
      "Epoch: 8190, Training Loss: 0.10998013615608215, Test Loss: 0.10170266777276993\n",
      "Epoch: 8200, Training Loss: 0.10993805527687073, Test Loss: 0.10166210681200027\n",
      "Epoch: 8210, Training Loss: 0.10989607125520706, Test Loss: 0.10162165015935898\n",
      "Epoch: 8220, Training Loss: 0.10985416173934937, Test Loss: 0.10158126056194305\n",
      "Epoch: 8230, Training Loss: 0.10981231927871704, Test Loss: 0.10154098272323608\n",
      "Epoch: 8240, Training Loss: 0.10977055132389069, Test Loss: 0.10150076448917389\n",
      "Epoch: 8250, Training Loss: 0.1097288653254509, Test Loss: 0.10146065056324005\n",
      "Epoch: 8260, Training Loss: 0.10968726128339767, Test Loss: 0.10142060369253159\n",
      "Epoch: 8270, Training Loss: 0.10964571684598923, Test Loss: 0.10138063877820969\n",
      "Epoch: 8280, Training Loss: 0.10960425436496735, Test Loss: 0.10134076327085495\n",
      "Epoch: 8290, Training Loss: 0.10956287384033203, Test Loss: 0.10130096971988678\n",
      "Epoch: 8300, Training Loss: 0.10952156782150269, Test Loss: 0.10126126557588577\n",
      "Epoch: 8310, Training Loss: 0.10948032885789871, Test Loss: 0.10122162848711014\n",
      "Epoch: 8320, Training Loss: 0.1094391718506813, Test Loss: 0.10118208080530167\n",
      "Epoch: 8330, Training Loss: 0.10939808934926987, Test Loss: 0.10114262253046036\n",
      "Epoch: 8340, Training Loss: 0.1093570739030838, Test Loss: 0.10110324621200562\n",
      "Epoch: 8350, Training Loss: 0.1093161329627037, Test Loss: 0.10106394439935684\n",
      "Epoch: 8360, Training Loss: 0.10927526652812958, Test Loss: 0.10102471709251404\n",
      "Epoch: 8370, Training Loss: 0.10923446714878082, Test Loss: 0.1009855791926384\n",
      "Epoch: 8380, Training Loss: 0.10919377207756042, Test Loss: 0.10094651579856873\n",
      "Epoch: 8390, Training Loss: 0.10915311425924301, Test Loss: 0.10090753436088562\n",
      "Epoch: 8400, Training Loss: 0.10911254584789276, Test Loss: 0.10086864233016968\n",
      "Epoch: 8410, Training Loss: 0.10907204449176788, Test Loss: 0.1008298322558403\n",
      "Epoch: 8420, Training Loss: 0.10903161019086838, Test Loss: 0.1007910966873169\n",
      "Epoch: 8430, Training Loss: 0.10899125784635544, Test Loss: 0.10075245052576065\n",
      "Epoch: 8440, Training Loss: 0.10895098000764847, Test Loss: 0.10071386396884918\n",
      "Epoch: 8450, Training Loss: 0.10891075432300568, Test Loss: 0.10067533701658249\n",
      "Epoch: 8460, Training Loss: 0.10887063294649124, Test Loss: 0.10063693672418594\n",
      "Epoch: 8470, Training Loss: 0.10883055627346039, Test Loss: 0.10059858113527298\n",
      "Epoch: 8480, Training Loss: 0.1087905615568161, Test Loss: 0.10056031495332718\n",
      "Epoch: 8490, Training Loss: 0.10875062644481659, Test Loss: 0.10052211582660675\n",
      "Epoch: 8500, Training Loss: 0.10871077328920364, Test Loss: 0.10048402100801468\n",
      "Epoch: 8510, Training Loss: 0.10867097973823547, Test Loss: 0.10044597834348679\n",
      "Epoch: 8520, Training Loss: 0.10863126814365387, Test Loss: 0.10040801763534546\n",
      "Epoch: 8530, Training Loss: 0.10859163105487823, Test Loss: 0.1003701463341713\n",
      "Epoch: 8540, Training Loss: 0.10855204612016678, Test Loss: 0.1003323420882225\n",
      "Epoch: 8550, Training Loss: 0.1085125282406807, Test Loss: 0.10029461234807968\n",
      "Epoch: 8560, Training Loss: 0.10847310721874237, Test Loss: 0.10025697201490402\n",
      "Epoch: 8570, Training Loss: 0.10843373090028763, Test Loss: 0.10021938383579254\n",
      "Epoch: 8580, Training Loss: 0.10839442908763885, Test Loss: 0.10018189996480942\n",
      "Epoch: 8590, Training Loss: 0.10835520923137665, Test Loss: 0.10014447569847107\n",
      "Epoch: 8600, Training Loss: 0.10831603407859802, Test Loss: 0.10010712593793869\n",
      "Epoch: 8610, Training Loss: 0.10827694833278656, Test Loss: 0.10006986558437347\n",
      "Epoch: 8620, Training Loss: 0.10823792219161987, Test Loss: 0.10003265738487244\n",
      "Epoch: 8630, Training Loss: 0.10819895565509796, Test Loss: 0.09999553859233856\n",
      "Epoch: 8640, Training Loss: 0.10816006362438202, Test Loss: 0.09995848685503006\n",
      "Epoch: 8650, Training Loss: 0.10812124609947205, Test Loss: 0.09992150962352753\n",
      "Epoch: 8660, Training Loss: 0.10808249562978745, Test Loss: 0.09988462179899216\n",
      "Epoch: 8670, Training Loss: 0.10804379731416702, Test Loss: 0.09984779357910156\n",
      "Epoch: 8680, Training Loss: 0.10800518095493317, Test Loss: 0.09981102496385574\n",
      "Epoch: 8690, Training Loss: 0.10796663165092468, Test Loss: 0.09977434575557709\n",
      "Epoch: 8700, Training Loss: 0.10792813450098038, Test Loss: 0.0997377336025238\n",
      "Epoch: 8710, Training Loss: 0.10788971930742264, Test Loss: 0.09970119595527649\n",
      "Epoch: 8720, Training Loss: 0.10785137116909027, Test Loss: 0.09966471791267395\n",
      "Epoch: 8730, Training Loss: 0.10781308263540268, Test Loss: 0.09962836652994156\n",
      "Epoch: 8740, Training Loss: 0.10777485370635986, Test Loss: 0.09959203004837036\n",
      "Epoch: 8750, Training Loss: 0.10773669183254242, Test Loss: 0.09955578297376633\n",
      "Epoch: 8760, Training Loss: 0.10769861191511154, Test Loss: 0.09951960295438766\n",
      "Epoch: 8770, Training Loss: 0.10766058415174484, Test Loss: 0.09948350489139557\n",
      "Epoch: 8780, Training Loss: 0.10762260854244232, Test Loss: 0.09944748133420944\n",
      "Epoch: 8790, Training Loss: 0.10758470743894577, Test Loss: 0.0994115099310875\n",
      "Epoch: 8800, Training Loss: 0.10754688084125519, Test Loss: 0.09937562793493271\n",
      "Epoch: 8810, Training Loss: 0.10750912874937057, Test Loss: 0.0993398055434227\n",
      "Epoch: 8820, Training Loss: 0.10747142136096954, Test Loss: 0.09930405765771866\n",
      "Epoch: 8830, Training Loss: 0.10743377357721329, Test Loss: 0.09926837682723999\n",
      "Epoch: 8840, Training Loss: 0.107396200299263, Test Loss: 0.0992327630519867\n",
      "Epoch: 8850, Training Loss: 0.10735868662595749, Test Loss: 0.09919722378253937\n",
      "Epoch: 8860, Training Loss: 0.10732124000787735, Test Loss: 0.09916175156831741\n",
      "Epoch: 8870, Training Loss: 0.10728385299444199, Test Loss: 0.09912634640932083\n",
      "Epoch: 8880, Training Loss: 0.107246533036232, Test Loss: 0.09909102320671082\n",
      "Epoch: 8890, Training Loss: 0.10720926523208618, Test Loss: 0.09905576705932617\n",
      "Epoch: 8900, Training Loss: 0.10717207193374634, Test Loss: 0.0990205705165863\n",
      "Epoch: 8910, Training Loss: 0.10713495314121246, Test Loss: 0.0989854484796524\n",
      "Epoch: 8920, Training Loss: 0.10709787160158157, Test Loss: 0.09895038604736328\n",
      "Epoch: 8930, Training Loss: 0.10706087201833725, Test Loss: 0.09891538321971893\n",
      "Epoch: 8940, Training Loss: 0.1070239245891571, Test Loss: 0.09888046979904175\n",
      "Epoch: 8950, Training Loss: 0.10698703676462173, Test Loss: 0.09884560853242874\n",
      "Epoch: 8960, Training Loss: 0.10695022344589233, Test Loss: 0.0988108292222023\n",
      "Epoch: 8970, Training Loss: 0.10691346228122711, Test Loss: 0.09877610951662064\n",
      "Epoch: 8980, Training Loss: 0.10687676072120667, Test Loss: 0.09874146431684494\n",
      "Epoch: 8990, Training Loss: 0.106840118765831, Test Loss: 0.09870686382055283\n",
      "Epoch: 9000, Training Loss: 0.1068035364151001, Test Loss: 0.09867236018180847\n",
      "Epoch: 9010, Training Loss: 0.10676702857017517, Test Loss: 0.0986379012465477\n",
      "Epoch: 9020, Training Loss: 0.10673059523105621, Test Loss: 0.0986035019159317\n",
      "Epoch: 9030, Training Loss: 0.10669417679309845, Test Loss: 0.09856919944286346\n",
      "Epoch: 9040, Training Loss: 0.10665784776210785, Test Loss: 0.09853493422269821\n",
      "Epoch: 9050, Training Loss: 0.10662157833576202, Test Loss: 0.09850075095891953\n",
      "Epoch: 9060, Training Loss: 0.10658536106348038, Test Loss: 0.09846661239862442\n",
      "Epoch: 9070, Training Loss: 0.1065492108464241, Test Loss: 0.09843256324529648\n",
      "Epoch: 9080, Training Loss: 0.1065131202340126, Test Loss: 0.09839856624603271\n",
      "Epoch: 9090, Training Loss: 0.10647708177566528, Test Loss: 0.09836464375257492\n",
      "Epoch: 9100, Training Loss: 0.10644110292196274, Test Loss: 0.0983307808637619\n",
      "Epoch: 9110, Training Loss: 0.10640519112348557, Test Loss: 0.09829698503017426\n",
      "Epoch: 9120, Training Loss: 0.10636934638023376, Test Loss: 0.09826325625181198\n",
      "Epoch: 9130, Training Loss: 0.10633354634046555, Test Loss: 0.09822957217693329\n",
      "Epoch: 9140, Training Loss: 0.1062978133559227, Test Loss: 0.09819597005844116\n",
      "Epoch: 9150, Training Loss: 0.10626211762428284, Test Loss: 0.09816242754459381\n",
      "Epoch: 9160, Training Loss: 0.10622651129961014, Test Loss: 0.09812895208597183\n",
      "Epoch: 9170, Training Loss: 0.10619094222784042, Test Loss: 0.09809552878141403\n",
      "Epoch: 9180, Training Loss: 0.10615544021129608, Test Loss: 0.09806216508150101\n",
      "Epoch: 9190, Training Loss: 0.10611999779939651, Test Loss: 0.09802888333797455\n",
      "Epoch: 9200, Training Loss: 0.10608459264039993, Test Loss: 0.09799566119909286\n",
      "Epoch: 9210, Training Loss: 0.10604927688837051, Test Loss: 0.09796249866485596\n",
      "Epoch: 9220, Training Loss: 0.10601400583982468, Test Loss: 0.09792938828468323\n",
      "Epoch: 9230, Training Loss: 0.10597878694534302, Test Loss: 0.09789635986089706\n",
      "Epoch: 9240, Training Loss: 0.10594363510608673, Test Loss: 0.09786337614059448\n",
      "Epoch: 9250, Training Loss: 0.10590852051973343, Test Loss: 0.09783044457435608\n",
      "Epoch: 9260, Training Loss: 0.1058734729886055, Test Loss: 0.09779760241508484\n",
      "Epoch: 9270, Training Loss: 0.10583849251270294, Test Loss: 0.09776481240987778\n",
      "Epoch: 9280, Training Loss: 0.10580357164144516, Test Loss: 0.0977320745587349\n",
      "Epoch: 9290, Training Loss: 0.10576869547367096, Test Loss: 0.09769941121339798\n",
      "Epoch: 9300, Training Loss: 0.10573387145996094, Test Loss: 0.09766680747270584\n",
      "Epoch: 9310, Training Loss: 0.10569910705089569, Test Loss: 0.09763425588607788\n",
      "Epoch: 9320, Training Loss: 0.10566440969705582, Test Loss: 0.0976017564535141\n",
      "Epoch: 9330, Training Loss: 0.10562974214553833, Test Loss: 0.09756933897733688\n",
      "Epoch: 9340, Training Loss: 0.10559515655040741, Test Loss: 0.09753697365522385\n",
      "Epoch: 9350, Training Loss: 0.10556062310934067, Test Loss: 0.09750466048717499\n",
      "Epoch: 9360, Training Loss: 0.10552613437175751, Test Loss: 0.0974724143743515\n",
      "Epoch: 9370, Training Loss: 0.10549170523881912, Test Loss: 0.09744022786617279\n",
      "Epoch: 9380, Training Loss: 0.10545732080936432, Test Loss: 0.09740810096263885\n",
      "Epoch: 9390, Training Loss: 0.10542301833629608, Test Loss: 0.0973760336637497\n",
      "Epoch: 9400, Training Loss: 0.10538875311613083, Test Loss: 0.09734403342008591\n",
      "Epoch: 9410, Training Loss: 0.10535453259944916, Test Loss: 0.0973120853304863\n",
      "Epoch: 9420, Training Loss: 0.10532038658857346, Test Loss: 0.09728018194437027\n",
      "Epoch: 9430, Training Loss: 0.10528628528118134, Test Loss: 0.09724836051464081\n",
      "Epoch: 9440, Training Loss: 0.10525224357843399, Test Loss: 0.09721659868955612\n",
      "Epoch: 9450, Training Loss: 0.10521826148033142, Test Loss: 0.09718488156795502\n",
      "Epoch: 9460, Training Loss: 0.10518430918455124, Test Loss: 0.09715323150157928\n",
      "Epoch: 9470, Training Loss: 0.10515043139457703, Test Loss: 0.09712162613868713\n",
      "Epoch: 9480, Training Loss: 0.1051165908575058, Test Loss: 0.09709008783102036\n",
      "Epoch: 9490, Training Loss: 0.10508281737565994, Test Loss: 0.09705859422683716\n",
      "Epoch: 9500, Training Loss: 0.10504909604787827, Test Loss: 0.09702717512845993\n",
      "Epoch: 9510, Training Loss: 0.10501543432474136, Test Loss: 0.09699582308530807\n",
      "Epoch: 9520, Training Loss: 0.10498180985450745, Test Loss: 0.09696450084447861\n",
      "Epoch: 9530, Training Loss: 0.1049482524394989, Test Loss: 0.09693325310945511\n",
      "Epoch: 9540, Training Loss: 0.10491473972797394, Test Loss: 0.09690205752849579\n",
      "Epoch: 9550, Training Loss: 0.10488127171993256, Test Loss: 0.09687091410160065\n",
      "Epoch: 9560, Training Loss: 0.10484787076711655, Test Loss: 0.09683983772993088\n",
      "Epoch: 9570, Training Loss: 0.10481452196836472, Test Loss: 0.09680882096290588\n",
      "Epoch: 9580, Training Loss: 0.10478121787309647, Test Loss: 0.09677784144878387\n",
      "Epoch: 9590, Training Loss: 0.1047479659318924, Test Loss: 0.09674693644046783\n",
      "Epoch: 9600, Training Loss: 0.1047147661447525, Test Loss: 0.09671606868505478\n",
      "Epoch: 9610, Training Loss: 0.10468162596225739, Test Loss: 0.0966852679848671\n",
      "Epoch: 9620, Training Loss: 0.10464853793382645, Test Loss: 0.09665452688932419\n",
      "Epoch: 9630, Training Loss: 0.10461549460887909, Test Loss: 0.09662384539842606\n",
      "Epoch: 9640, Training Loss: 0.10458249598741531, Test Loss: 0.0965932160615921\n",
      "Epoch: 9650, Training Loss: 0.10454955697059631, Test Loss: 0.09656263887882233\n",
      "Epoch: 9660, Training Loss: 0.10451667010784149, Test Loss: 0.09653211385011673\n",
      "Epoch: 9670, Training Loss: 0.10448384284973145, Test Loss: 0.09650164097547531\n",
      "Epoch: 9680, Training Loss: 0.10445106029510498, Test Loss: 0.09647122770547867\n",
      "Epoch: 9690, Training Loss: 0.1044183149933815, Test Loss: 0.0964408591389656\n",
      "Epoch: 9700, Training Loss: 0.1043856292963028, Test Loss: 0.09641056507825851\n",
      "Epoch: 9710, Training Loss: 0.10435299575328827, Test Loss: 0.096380315721035\n",
      "Epoch: 9720, Training Loss: 0.10432040691375732, Test Loss: 0.09635011106729507\n",
      "Epoch: 9730, Training Loss: 0.10428787767887115, Test Loss: 0.09631998836994171\n",
      "Epoch: 9740, Training Loss: 0.10425539314746857, Test Loss: 0.09628988802433014\n",
      "Epoch: 9750, Training Loss: 0.10422296822071075, Test Loss: 0.09625986963510513\n",
      "Epoch: 9760, Training Loss: 0.10419057309627533, Test Loss: 0.09622988849878311\n",
      "Epoch: 9770, Training Loss: 0.10415824502706528, Test Loss: 0.09619994461536407\n",
      "Epoch: 9780, Training Loss: 0.10412594676017761, Test Loss: 0.09617007523775101\n",
      "Epoch: 9790, Training Loss: 0.10409373044967651, Test Loss: 0.09614025056362152\n",
      "Epoch: 9800, Training Loss: 0.1040615439414978, Test Loss: 0.09611048549413681\n",
      "Epoch: 9810, Training Loss: 0.10402940213680267, Test Loss: 0.09608077257871628\n",
      "Epoch: 9820, Training Loss: 0.10399730503559113, Test Loss: 0.09605110436677933\n",
      "Epoch: 9830, Training Loss: 0.10396527498960495, Test Loss: 0.09602149575948715\n",
      "Epoch: 9840, Training Loss: 0.10393328964710236, Test Loss: 0.09599194675683975\n",
      "Epoch: 9850, Training Loss: 0.10390134155750275, Test Loss: 0.09596244245767593\n",
      "Epoch: 9860, Training Loss: 0.10386945307254791, Test Loss: 0.09593300521373749\n",
      "Epoch: 9870, Training Loss: 0.10383762419223785, Test Loss: 0.09590359032154083\n",
      "Epoch: 9880, Training Loss: 0.10380581021308899, Test Loss: 0.09587424993515015\n",
      "Epoch: 9890, Training Loss: 0.1037740707397461, Test Loss: 0.09584492444992065\n",
      "Epoch: 9900, Training Loss: 0.10374238342046738, Test Loss: 0.09581571072340012\n",
      "Epoch: 9910, Training Loss: 0.10371071845293045, Test Loss: 0.09578651934862137\n",
      "Epoch: 9920, Training Loss: 0.1036791205406189, Test Loss: 0.09575735777616501\n",
      "Epoch: 9930, Training Loss: 0.10364756733179092, Test Loss: 0.09572827816009521\n",
      "Epoch: 9940, Training Loss: 0.10361605137586594, Test Loss: 0.0956992357969284\n",
      "Epoch: 9950, Training Loss: 0.10358460247516632, Test Loss: 0.09567024558782578\n",
      "Epoch: 9960, Training Loss: 0.10355318337678909, Test Loss: 0.09564130008220673\n",
      "Epoch: 9970, Training Loss: 0.10352182388305664, Test Loss: 0.09561242908239365\n",
      "Epoch: 9980, Training Loss: 0.10349051654338837, Test Loss: 0.09558358788490295\n",
      "Epoch: 9990, Training Loss: 0.10345923155546188, Test Loss: 0.09555480629205704\n",
      "Epoch: 10000, Training Loss: 0.10342801362276077, Test Loss: 0.0955260694026947\n"
     ]
    }
   ],
   "source": [
    "# Train the model and validate it\n",
    "num_epochs = 10000\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(train_data.float())\n",
    "    y_pred = y_pred.squeeze()\n",
    "    loss_value = loss(y_pred, train_target.float())\n",
    "    loss_value.backward()\n",
    "    optimizer.step()\n",
    "    train_loss.append(loss_value.item())\n",
    "\n",
    "    model.eval()\n",
    "    y_pred = model(test_data.float())\n",
    "    y_pred = y_pred.squeeze()\n",
    "    loss_value = loss(y_pred, test_target.float())\n",
    "    test_loss.append(loss_value.item())\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch: {epoch + 1}, Training Loss: {train_loss[-1]}, Test Loss: {test_loss[-1]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the training and test loss\n",
    "\n",
    "plt.plot(train_loss, label='Training Loss')\n",
    "plt.plot(test_loss, label='Test Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9649122953414917\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy of the model\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(test_data.float())\n",
    "    y_pred = y_pred.squeeze()\n",
    "    y_pred = torch.round(y_pred) # round the output to 0 or 1\n",
    "    accuracy = (y_pred == test_target).sum() / test_target.shape[0]\n",
    "    print(f'Accuracy: {accuracy.item()}')\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'breast_cancer_model.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
